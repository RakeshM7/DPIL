{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RakeshM7/DPIL-BERT/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "d02567bc-2cff-4ce5-b331-aab505f31522"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "55834442-8ddb-49a2-b7b8-329ca4ff9c60"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4udGSQsHzAc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "36577438-787c-486e-b567-903694e929c0"
      },
      "source": [
        "file_path= input(\"Enter the input file name : \")    #example - task1hindi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the input file name : task1hindi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "3e8a1214-15b1-4a05-ee72-a98ed021606f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/hindi/\"+file_path+\".tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1229</th>\n",
              "      <td>HIN1230</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>दो इतालवी नौसैनिकों को 2012 में दो मछुआरों को ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>HIN0018</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>उन के मुताबिक  हमारी रिपोर्ट में किसी को क्लीन...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>HIN0893</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>कन्हैया कुमार पर फिर हमला, प्लेन में एक शख्स न...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2078</th>\n",
              "      <td>HIN2079</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>हेलीकॉप्टर पहले भीमताल से पानी भरेगा और फिर नै...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>HIN0496</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>फ्लाईओवर हादसे के लिए किसी कमी-खामी को जिम्मेद...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>HIN0797</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>नोएडा में दिनदहाड़े गुंडों ने की पुलिसवाले की ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>HIN1318</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>फाइनल वर्जन बनाने में दस से पन्द्रह साल लगेंगे...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>HIN0055</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>पूर्वोत्तर में भूकंप : मणिपुर में सात  मरे, सौ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>727</th>\n",
              "      <td>HIN0728</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>उत्तराखंड मामले में हाईकोर्ट का यह कहना है कि ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>HIN0347</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>सौरभ सेनगुप्ता द्वारा निर्देशित 'इट्स मैन्स वर...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "1229         HIN1230  ...  दो इतालवी नौसैनिकों को 2012 में दो मछुआरों को ...\n",
              "17           HIN0018  ...  उन के मुताबिक  हमारी रिपोर्ट में किसी को क्लीन...\n",
              "892          HIN0893  ...  कन्हैया कुमार पर फिर हमला, प्लेन में एक शख्स न...\n",
              "2078         HIN2079  ...  हेलीकॉप्टर पहले भीमताल से पानी भरेगा और फिर नै...\n",
              "495          HIN0496  ...  फ्लाईओवर हादसे के लिए किसी कमी-खामी को जिम्मेद...\n",
              "796          HIN0797  ...  नोएडा में दिनदहाड़े गुंडों ने की पुलिसवाले की ...\n",
              "1317         HIN1318  ...  फाइनल वर्जन बनाने में दस से पन्द्रह साल लगेंगे...\n",
              "54           HIN0055  ...  पूर्वोत्तर में भूकंप : मणिपुर में सात  मरे, सौ...\n",
              "727          HIN0728  ...  उत्तराखंड मामले में हाईकोर्ट का यह कहना है कि ...\n",
              "346          HIN0347  ...  सौरभ सेनगुप्ता द्वारा निर्देशित 'इट्स मैन्स वर...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c7d53aac-963c-4983-b855-e51a6099de0b"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1297</th>\n",
              "      <td>यह तारीख सतायीस दिसंबर होगी,इसी दिन सुपरस्‍टार...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1579</th>\n",
              "      <td>मजबूत टीम इंडिया को वर्ल्ड टी ट्वन्टि  के सेमी...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1753</th>\n",
              "      <td>टेनिस स्टार को गत वर्ष देश के सर्वोच्च खेल पुर...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1879</th>\n",
              "      <td>दिल्ली के  नैचरल हिस्ट्री म्यूजियम में भीषण आग...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1708</th>\n",
              "      <td>बीजेपी-पीडीपी की इस सरकार में सोलह  कैबिनेट और...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1297  यह तारीख सतायीस दिसंबर होगी,इसी दिन सुपरस्‍टार...      0\n",
              "1579  मजबूत टीम इंडिया को वर्ल्ड टी ट्वन्टि  के सेमी...      0\n",
              "1753  टेनिस स्टार को गत वर्ष देश के सर्वोच्च खेल पुर...      0\n",
              "1879  दिल्ली के  नैचरल हिस्ट्री म्यूजियम में भीषण आग...      0\n",
              "1708  बीजेपी-पीडीपी की इस सरकार में सोलह  कैबिनेट और...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dbb565f7-de69-4a8a-a82f-312c919ba885"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "6c4875fb-5733-4197-ab55-2f7d438e1a85"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  भारतीय मुस्लिमों की वजह से नहीं पनप सकता आईएस|<eol>भारत में कभी वर्चस्व कायम नहीं कर सकता आईएस|\n",
            "Tokenized:  ['भारतीय', 'म', '##स', '##ल', '##िम', '##ो', 'की', 'व', '##ज', '##ह', 'स', 'न', '##ही', 'प', '##न', '##प', 'सकता', 'आई', '##ए', '##स', '|', '<', 'eo', '##l', '>', 'भारत', 'म', 'कभी', 'व', '##र', '##च', '##स', '##व', 'का', '##यम', 'न', '##ही', 'कर', 'सकता', 'आई', '##ए', '##स', '|']\n",
            "Token IDs:  [18725, 889, 13432, 11714, 50419, 13718, 10826, 895, 17413, 17110, 898, 884, 24667, 885, 11453, 18187, 26886, 44881, 22599, 13432, 196, 133, 13934, 10161, 135, 14311, 889, 50058, 895, 11549, 16940, 13432, 15070, 11081, 87136, 884, 24667, 16192, 26886, 44881, 22599, 13432, 196]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "0109f997-d45b-4372-8d0d-0d52edb7091b"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  भारतीय मुस्लिमों की वजह से नहीं पनप सकता आईएस|<eol>भारत में कभी वर्चस्व कायम नहीं कर सकता आईएस|\n",
            "Token IDs: [101, 18725, 889, 13432, 11714, 50419, 13718, 10826, 895, 17413, 17110, 898, 884, 24667, 885, 11453, 18187, 26886, 44881, 22599, 13432, 196, 133, 13934, 10161, 135, 14311, 889, 50058, 895, 11549, 16940, 13432, 15070, 11081, 87136, 884, 24667, 16192, 26886, 44881, 22599, 13432, 196, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9394495e-8b0e-43c4-840b-3fb8c097db25"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "88728356-fc7c-4305-8b23-0dbf2b0e57d9"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "2e9ccae6-6092-4a6a-d4ad-f6cc4b643e93"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101    881  11549 ...      0      0      0]\n",
            " [   101    882  27391 ...      0      0      0]\n",
            " [   101    899  15552 ...  10826    889  78530]\n",
            " ...\n",
            " [   101    882  27391 ...  14265    886  13432]\n",
            " [   101    882  27391 ...    884 100915  16380]\n",
            " [   101    889  18351 ...      0      0      0]]\n",
            "[1 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c08d072-a277-4e0a-f0e8-fe430b79be35"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "adec116c-bbdd-4a6b-ec2d-8e062bf9368c"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "20261c7e-a4f2-4576-cb41-7cecd00aae9d"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:15.\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epcoh took: 0:00:23\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:15.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:15.\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epcoh took: 0:00:24\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:16.\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epcoh took: 0:00:25\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Er8Tzb9xj-D",
        "colab_type": "text"
      },
      "source": [
        "Uncomment if you want graphical representaion of training vs epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifs2QgT9HeUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# % matplotlib inline\n",
        "\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Use plot styling from seaborn.\n",
        "# sns.set(style='darkgrid')\n",
        "\n",
        "# # Increase the plot size and font size.\n",
        "# sns.set(font_scale=1.5)\n",
        "# plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# # Plot the learning curve.\n",
        "# plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# # Label the plot.\n",
        "# plt.title(\"Training loss\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJBFn_4nziAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "481ba2c3-bfb3-44b7-c61d-59dc6f0bdb92"
      },
      "source": [
        "TestData_name = input(\"Enter the name of the test data tsv file: \")     #example - task1hindi-test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the name of the test data tsv file: task1hindi-test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b89e54c2-b807-4383-9e2a-7639985b9eb1"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/\"+TestData_name+\".tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ddfe993e-8bbf-4bb2-df11-63e3951bd6a5"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "#print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 900 test sentences...\n",
            "    DONE.\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaKraEhj0OxL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9440282b-5d2d-449c-8f16-7319b425bc3c"
      },
      "source": [
        "Results_path = input(\"Enter the filename of the result file : \")  #example - task1hindi-results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the filename of the result file : task1hindi-results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/\"+Results_path+\".txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5bfbde16-8599-4e53-e277-f41489c6ccaf"
      },
      "source": [
        "\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t [-1.4468662  2.0582027] \t1\ttrue\n",
            "(0)\t 2\t [ 1.4242511 -1.8237956] \t0\ttrue\n",
            "(1)\t 3\t [-1.6872741  2.2216592] \t1\ttrue\n",
            "(0)\t 4\t [ 1.7798494 -2.0529678] \t0\ttrue\n",
            "(0)\t 5\t [ 1.9186412 -2.3068173] \t0\ttrue\n",
            "(1)\t 6\t [-1.4315914  1.5681046] \t1\ttrue\n",
            "(0)\t 7\t [ 1.7652569 -2.2170978] \t0\ttrue\n",
            "(0)\t 8\t [ 1.5669459 -2.0596282] \t0\ttrue\n",
            "(1)\t 9\t [-1.4323771  1.6051176] \t1\ttrue\n",
            "(0)\t 10\t [ 1.5550834 -2.057265 ] \t0\ttrue\n",
            "(0)\t 11\t [ 1.3785862 -1.7210454] \t0\ttrue\n",
            "(1)\t 12\t [-0.5210194  0.353309 ] \t1\ttrue\n",
            "(0)\t 13\t [ 2.1268418 -2.5518117] \t0\ttrue\n",
            "(0)\t 14\t [ 1.8890624 -2.1804905] \t0\ttrue\n",
            "(1)\t 15\t [-1.4152423  2.046114 ] \t1\ttrue\n",
            "(0)\t 16\t [-1.4259226  1.4930948] \t1\tfalse\n",
            "(0)\t 17\t [-0.31130743  0.02605255] \t1\tfalse\n",
            "(0)\t 18\t [ 1.5100602 -1.9908589] \t0\ttrue\n",
            "(1)\t 19\t [-1.4287342  2.0433774] \t1\ttrue\n",
            "(1)\t 20\t [-1.6782357  2.0704408] \t1\ttrue\n",
            "(0)\t 21\t [ 0.22940652 -0.6770026 ] \t0\ttrue\n",
            "(0)\t 22\t [ 2.0147347 -2.5035331] \t0\ttrue\n",
            "(0)\t 23\t [ 0.83285713 -1.2709394 ] \t0\ttrue\n",
            "(1)\t 24\t [-1.7375194  2.245208 ] \t1\ttrue\n",
            "(0)\t 25\t [ 1.8513414 -2.334033 ] \t0\ttrue\n",
            "(0)\t 26\t [ 1.7569305 -2.2301996] \t0\ttrue\n",
            "(1)\t 27\t [-1.4636024  2.055412 ] \t1\ttrue\n",
            "(0)\t 28\t [ 1.386178  -1.7221704] \t0\ttrue\n",
            "(0)\t 29\t [ 2.0366087 -2.4615061] \t0\ttrue\n",
            "(1)\t 30\t [-1.4038224  1.9976774] \t1\ttrue\n",
            "(1)\t 31\t [-1.6183175  2.0119674] \t1\ttrue\n",
            "(0)\t 32\t [ 2.086824  -2.5201735] \t0\ttrue\n",
            "(1)\t 33\t [-1.5832263  1.8685702] \t1\ttrue\n",
            "(0)\t 34\t [-0.543834   0.3405484] \t1\tfalse\n",
            "(0)\t 35\t [ 1.737943  -2.2256255] \t0\ttrue\n",
            "(0)\t 36\t [ 1.9450234 -2.443164 ] \t0\ttrue\n",
            "(0)\t 37\t [ 0.3951608  -0.56877726] \t0\ttrue\n",
            "(1)\t 38\t [-1.719561  2.140671] \t1\ttrue\n",
            "(0)\t 39\t [ 1.8078187 -2.4491396] \t0\ttrue\n",
            "(1)\t 40\t [-1.6329381  2.1038947] \t1\ttrue\n",
            "(0)\t 41\t [ 0.7120649 -1.0191152] \t0\ttrue\n",
            "(1)\t 42\t [-1.5418241  2.0902424] \t1\ttrue\n",
            "(0)\t 43\t [-0.4668394  0.2177893] \t1\tfalse\n",
            "(0)\t 44\t [ 1.4818329 -1.9368929] \t0\ttrue\n",
            "(0)\t 45\t [ 1.0853766 -1.4475797] \t0\ttrue\n",
            "(0)\t 46\t [ 0.46161398 -0.6194045 ] \t0\ttrue\n",
            "(1)\t 47\t [-1.2579796  1.7901418] \t1\ttrue\n",
            "(1)\t 48\t [-1.4934536  1.9779433] \t1\ttrue\n",
            "(0)\t 49\t [-0.9871915   0.95105463] \t1\tfalse\n",
            "(0)\t 50\t [ 2.0024781 -2.50247  ] \t0\ttrue\n",
            "(1)\t 51\t [-1.753142   2.2972496] \t1\ttrue\n",
            "(0)\t 52\t [ 2.2376628 -2.5886426] \t0\ttrue\n",
            "(1)\t 53\t [-1.0068973  1.1266836] \t1\ttrue\n",
            "(1)\t 54\t [-1.6472809  2.2756598] \t1\ttrue\n",
            "(0)\t 55\t [ 1.8513973 -2.382977 ] \t0\ttrue\n",
            "(0)\t 56\t [ 1.5920553 -1.8776972] \t0\ttrue\n",
            "(0)\t 57\t [ 1.5296409 -1.9137262] \t0\ttrue\n",
            "(0)\t 58\t [ 1.7947961 -2.1106625] \t0\ttrue\n",
            "(1)\t 59\t [-1.7694323  2.207491 ] \t1\ttrue\n",
            "(0)\t 60\t [ 0.9111066 -1.2297602] \t0\ttrue\n",
            "(0)\t 61\t [ 1.8203942 -2.2904053] \t0\ttrue\n",
            "(1)\t 62\t [-1.6569173  1.9161512] \t1\ttrue\n",
            "(0)\t 63\t [-1.5105389  1.6161138] \t1\tfalse\n",
            "(1)\t 64\t [-1.3050523  1.8827881] \t1\ttrue\n",
            "(0)\t 65\t [ 1.8776695 -2.3502097] \t0\ttrue\n",
            "(0)\t 66\t [ 1.368718 -1.889181] \t0\ttrue\n",
            "(1)\t 67\t [-1.4310018  2.01486  ] \t1\ttrue\n",
            "(1)\t 68\t [-1.4367844  2.0140631] \t1\ttrue\n",
            "(0)\t 69\t [-1.3559036  1.5420598] \t1\tfalse\n",
            "(0)\t 70\t [ 1.8896456 -2.3079517] \t0\ttrue\n",
            "(1)\t 71\t [-1.3831222  1.894918 ] \t1\ttrue\n",
            "(1)\t 72\t [-1.2140145  1.8274889] \t1\ttrue\n",
            "(0)\t 73\t [ 1.8149855 -2.3508027] \t0\ttrue\n",
            "(0)\t 74\t [ 1.5528316 -2.0526404] \t0\ttrue\n",
            "(1)\t 75\t [-1.7170899  2.1028116] \t1\ttrue\n",
            "(0)\t 76\t [ 1.7414154 -2.3440375] \t0\ttrue\n",
            "(1)\t 77\t [-1.647804   2.1307998] \t1\ttrue\n",
            "(0)\t 78\t [ 0.38845143 -0.43023336] \t0\ttrue\n",
            "(1)\t 79\t [-1.36984    1.9427034] \t1\ttrue\n",
            "(0)\t 80\t [ 1.9300396 -2.265131 ] \t0\ttrue\n",
            "(1)\t 81\t [-1.4490417  2.040117 ] \t1\ttrue\n",
            "(0)\t 82\t [ 1.4110966 -1.8129202] \t0\ttrue\n",
            "(0)\t 83\t [ 1.7653754 -2.3137732] \t0\ttrue\n",
            "(1)\t 84\t [-1.7368975  2.2088897] \t1\ttrue\n",
            "(0)\t 85\t [ 1.6702094 -2.1951625] \t0\ttrue\n",
            "(1)\t 86\t [ 0.20786695 -0.36819014] \t0\tfalse\n",
            "(1)\t 87\t [-1.6296278  2.1820261] \t1\ttrue\n",
            "(0)\t 88\t [ 2.0627825 -2.5875533] \t0\ttrue\n",
            "(1)\t 89\t [-0.72287136  0.5119326 ] \t1\ttrue\n",
            "(0)\t 90\t [-1.6641743  2.1326156] \t1\tfalse\n",
            "(0)\t 91\t [ 1.8658401 -2.490466 ] \t0\ttrue\n",
            "(1)\t 92\t [-1.7509198  2.3239052] \t1\ttrue\n",
            "(1)\t 93\t [-1.5139868  2.1073563] \t1\ttrue\n",
            "(0)\t 94\t [-0.5535445  0.3595675] \t1\tfalse\n",
            "(0)\t 95\t [ 1.3879267 -1.8238479] \t0\ttrue\n",
            "(1)\t 96\t [-1.7390336  2.2154415] \t1\ttrue\n",
            "(0)\t 97\t [ 1.615311 -2.101678] \t0\ttrue\n",
            "(1)\t 98\t [-1.6327591  2.2252154] \t1\ttrue\n",
            "(0)\t 99\t [ 0.19083178 -0.5788825 ] \t0\ttrue\n",
            "(1)\t 100\t [-0.24370128  0.45884475] \t1\ttrue\n",
            "(0)\t 101\t [-0.30085868  0.09257334] \t1\tfalse\n",
            "(1)\t 102\t [-0.2644201   0.07492977] \t1\ttrue\n",
            "(0)\t 103\t [-0.2644201   0.07492977] \t1\tfalse\n",
            "(0)\t 104\t [ 1.5303526 -1.7403016] \t0\ttrue\n",
            "(1)\t 105\t [-1.2990224  1.4898776] \t1\ttrue\n",
            "(1)\t 106\t [-1.6628771  2.092757 ] \t1\ttrue\n",
            "(0)\t 107\t [ 0.00637203 -0.354082  ] \t0\ttrue\n",
            "(0)\t 108\t [ 1.7971598 -2.1349578] \t0\ttrue\n",
            "(1)\t 109\t [-1.3650264  1.6642662] \t1\ttrue\n",
            "(0)\t 110\t [ 2.000117 -2.581222] \t0\ttrue\n",
            "(1)\t 111\t [-1.770747   2.2774374] \t1\ttrue\n",
            "(1)\t 112\t [ 0.03309359 -0.30067843] \t0\tfalse\n",
            "(0)\t 113\t [ 0.03309359 -0.30067843] \t0\ttrue\n",
            "(0)\t 114\t [ 1.3779327 -1.778922 ] \t0\ttrue\n",
            "(1)\t 115\t [-0.40997767  0.21489128] \t1\ttrue\n",
            "(1)\t 116\t [-1.5669541  1.7163175] \t1\ttrue\n",
            "(0)\t 117\t [ 1.9960151 -2.3951437] \t0\ttrue\n",
            "(0)\t 118\t [ 1.7336457 -2.3419955] \t0\ttrue\n",
            "(1)\t 119\t [-1.4781516  2.061863 ] \t1\ttrue\n",
            "(0)\t 120\t [ 1.9282879 -2.4858994] \t0\ttrue\n",
            "(1)\t 121\t [-1.0667546  1.6767339] \t1\ttrue\n",
            "(1)\t 122\t [-1.2786198  1.4935107] \t1\ttrue\n",
            "(0)\t 123\t [ 1.9733192 -2.5180097] \t0\ttrue\n",
            "(0)\t 124\t [ 1.7565192 -2.252649 ] \t0\ttrue\n",
            "(1)\t 125\t [-1.7039188  2.1134613] \t1\ttrue\n",
            "(1)\t 126\t [-1.694749  2.239396] \t1\ttrue\n",
            "(0)\t 127\t [ 1.9183515 -2.3700883] \t0\ttrue\n",
            "(0)\t 128\t [ 2.0044298 -2.2719982] \t0\ttrue\n",
            "(0)\t 129\t [ 1.7550291 -2.3165033] \t0\ttrue\n",
            "(0)\t 130\t [ 1.9624023 -2.48631  ] \t0\ttrue\n",
            "(1)\t 131\t [-1.379638   1.4808824] \t1\ttrue\n",
            "(0)\t 132\t [ 1.4791268 -1.8604642] \t0\ttrue\n",
            "(1)\t 133\t [-1.6286831  2.0934935] \t1\ttrue\n",
            "(0)\t 134\t [ 1.9826173 -2.5084434] \t0\ttrue\n",
            "(0)\t 135\t [ 1.8112742 -2.2283554] \t0\ttrue\n",
            "(1)\t 136\t [-1.726503   2.0550132] \t1\ttrue\n",
            "(1)\t 137\t [-1.6423899  2.0661793] \t1\ttrue\n",
            "(0)\t 138\t [ 1.4766024 -1.9493178] \t0\ttrue\n",
            "(1)\t 139\t [-1.5046684  2.01082  ] \t1\ttrue\n",
            "(0)\t 140\t [ 0.3194946 -0.6399218] \t0\ttrue\n",
            "(0)\t 141\t [ 1.8120469 -2.179514 ] \t0\ttrue\n",
            "(1)\t 142\t [-1.1688989  1.18244  ] \t1\ttrue\n",
            "(0)\t 143\t [ 0.9871333 -1.1285572] \t0\ttrue\n",
            "(1)\t 144\t [ 1.579142  -2.1093984] \t0\tfalse\n",
            "(0)\t 145\t [ 1.2897831 -1.7403331] \t0\ttrue\n",
            "(1)\t 146\t [-1.4352663  1.7403902] \t1\ttrue\n",
            "(0)\t 147\t [ 1.9680319 -2.42611  ] \t0\ttrue\n",
            "(1)\t 148\t [-1.6930052  2.3097284] \t1\ttrue\n",
            "(1)\t 149\t [-1.5318182  2.0004249] \t1\ttrue\n",
            "(0)\t 150\t [ 2.1458998 -2.5196934] \t0\ttrue\n",
            "(0)\t 151\t [ 1.760521 -2.305584] \t0\ttrue\n",
            "(1)\t 152\t [-1.4030062  1.9811692] \t1\ttrue\n",
            "(0)\t 153\t [ 1.1830219 -1.4743845] \t0\ttrue\n",
            "(0)\t 154\t [ 1.8135172 -2.2426558] \t0\ttrue\n",
            "(1)\t 155\t [-1.4138436  1.4876891] \t1\ttrue\n",
            "(0)\t 156\t [ 2.0004911 -2.5238593] \t0\ttrue\n",
            "(0)\t 157\t [ 0.05503236 -0.43825647] \t0\ttrue\n",
            "(1)\t 158\t [ 0.05503236 -0.43825647] \t0\tfalse\n",
            "(1)\t 159\t [-1.7390712  2.316727 ] \t1\ttrue\n",
            "(0)\t 160\t [-1.3536283  1.5378634] \t1\tfalse\n",
            "(0)\t 161\t [ 1.4320861 -1.9250422] \t0\ttrue\n",
            "(0)\t 162\t [-1.1834017  1.1244528] \t1\tfalse\n",
            "(1)\t 163\t [-1.7544923  2.1926363] \t1\ttrue\n",
            "(1)\t 164\t [-1.648903   2.1819656] \t1\ttrue\n",
            "(0)\t 165\t [ 2.0321007 -2.5305521] \t0\ttrue\n",
            "(1)\t 166\t [-1.6263764  2.1048498] \t1\ttrue\n",
            "(0)\t 167\t [ 1.8474377 -2.3702955] \t0\ttrue\n",
            "(1)\t 168\t [-1.7383521  2.0695522] \t1\ttrue\n",
            "(0)\t 169\t [ 1.9433718 -2.4023776] \t0\ttrue\n",
            "(0)\t 170\t [-0.03109657 -0.30380347] \t0\ttrue\n",
            "(1)\t 171\t [-0.03109657 -0.30380347] \t0\tfalse\n",
            "(0)\t 172\t [ 0.7284724 -1.1230224] \t0\ttrue\n",
            "(0)\t 173\t [ 1.9708852 -2.4548442] \t0\ttrue\n",
            "(1)\t 174\t [-1.4697456  2.0257325] \t1\ttrue\n",
            "(0)\t 175\t [ 1.8527901 -2.2185102] \t0\ttrue\n",
            "(0)\t 176\t [ 1.9342105 -2.4901881] \t0\ttrue\n",
            "(1)\t 177\t [-1.5669829  2.0029857] \t1\ttrue\n",
            "(1)\t 178\t [-1.591608   1.9049207] \t1\ttrue\n",
            "(0)\t 179\t [ 1.6421275 -2.130592 ] \t0\ttrue\n",
            "(1)\t 180\t [ 1.2831165 -1.6174158] \t0\tfalse\n",
            "(0)\t 181\t [ 1.5966567 -2.1192791] \t0\ttrue\n",
            "(0)\t 182\t [ 1.9334626 -2.5363526] \t0\ttrue\n",
            "(1)\t 183\t [-1.641693   2.1759105] \t1\ttrue\n",
            "(1)\t 184\t [ 1.0512847 -1.3152825] \t0\tfalse\n",
            "(0)\t 185\t [-1.0378054  1.0279026] \t1\tfalse\n",
            "(0)\t 186\t [-0.65577716  0.43032238] \t1\tfalse\n",
            "(1)\t 187\t [-1.7213728  2.1848369] \t1\ttrue\n",
            "(1)\t 188\t [-1.4455432  2.08135  ] \t1\ttrue\n",
            "(0)\t 189\t [ 1.9004657 -2.4373553] \t0\ttrue\n",
            "(0)\t 190\t [ 1.9736241 -2.5146434] \t0\ttrue\n",
            "(1)\t 191\t [-1.701297  2.265167] \t1\ttrue\n",
            "(1)\t 192\t [-1.6755035  2.0821981] \t1\ttrue\n",
            "(0)\t 193\t [ 1.0256504 -1.3811444] \t0\ttrue\n",
            "(1)\t 194\t [-1.4625236  1.7325814] \t1\ttrue\n",
            "(0)\t 195\t [-0.45487586  0.19557013] \t1\tfalse\n",
            "(1)\t 196\t [-1.5643833  2.1987362] \t1\ttrue\n",
            "(0)\t 197\t [ 1.7985228 -2.3276553] \t0\ttrue\n",
            "(1)\t 198\t [-1.6146955  2.2280777] \t1\ttrue\n",
            "(0)\t 199\t [ 1.4127321 -1.7544886] \t0\ttrue\n",
            "(0)\t 200\t [-0.19112948 -0.091534  ] \t1\tfalse\n",
            "(0)\t 201\t [ 1.0492013 -1.299462 ] \t0\ttrue\n",
            "(1)\t 202\t [-1.6837107  2.0632968] \t1\ttrue\n",
            "(1)\t 203\t [ 1.582533  -1.9942853] \t0\tfalse\n",
            "(0)\t 204\t [ 1.6065102 -2.0517335] \t0\ttrue\n",
            "(0)\t 205\t [ 2.1293147 -2.5503695] \t0\ttrue\n",
            "(1)\t 206\t [-1.4313463  1.9390267] \t1\ttrue\n",
            "(1)\t 207\t [-1.6923494  2.1671343] \t1\ttrue\n",
            "(0)\t 208\t [ 1.5160234 -2.0503793] \t0\ttrue\n",
            "(1)\t 209\t [-1.6619467  2.275194 ] \t1\ttrue\n",
            "(0)\t 210\t [ 1.6835926 -2.272746 ] \t0\ttrue\n",
            "(0)\t 211\t [ 0.7712602 -0.9122061] \t0\ttrue\n",
            "(1)\t 212\t [-1.6464245  2.141818 ] \t1\ttrue\n",
            "(1)\t 213\t [-1.7423797  2.2968323] \t1\ttrue\n",
            "(0)\t 214\t [ 2.047534  -2.3804164] \t0\ttrue\n",
            "(0)\t 215\t [-0.44186202  0.17034577] \t1\tfalse\n",
            "(1)\t 216\t [-1.3488479  1.4480392] \t1\ttrue\n",
            "(0)\t 217\t [-1.3290864  1.9379004] \t1\tfalse\n",
            "(1)\t 218\t [-1.4325207  1.7088177] \t1\ttrue\n",
            "(0)\t 219\t [-1.5643512  2.0585673] \t1\tfalse\n",
            "(1)\t 220\t [-1.3240404  1.4796737] \t1\ttrue\n",
            "(1)\t 221\t [-1.6106595  2.1946154] \t1\ttrue\n",
            "(0)\t 222\t [ 1.9590517 -2.4846737] \t0\ttrue\n",
            "(1)\t 223\t [-1.6431291  2.0417073] \t1\ttrue\n",
            "(0)\t 224\t [ 2.173681  -2.4428692] \t0\ttrue\n",
            "(0)\t 225\t [ 2.173681  -2.4428692] \t0\ttrue\n",
            "(1)\t 226\t [-0.99422497  0.93370223] \t1\ttrue\n",
            "(0)\t 227\t [-1.3707125  1.533936 ] \t1\tfalse\n",
            "(1)\t 228\t [-1.4505047  1.6530102] \t1\ttrue\n",
            "(1)\t 229\t [-0.204464   -0.06085764] \t1\ttrue\n",
            "(0)\t 230\t [-0.204464   -0.06085764] \t1\tfalse\n",
            "(0)\t 231\t [ 1.5111816 -2.0794659] \t0\ttrue\n",
            "(1)\t 232\t [-1.4704994  1.6943476] \t1\ttrue\n",
            "(1)\t 233\t [-1.4492589  1.7206969] \t1\ttrue\n",
            "(0)\t 234\t [-0.70166945  0.52875406] \t1\tfalse\n",
            "(1)\t 235\t [-1.5463192  1.773674 ] \t1\ttrue\n",
            "(0)\t 236\t [ 2.0075393 -2.5874274] \t0\ttrue\n",
            "(0)\t 237\t [-0.33296448  0.0896823 ] \t1\tfalse\n",
            "(1)\t 238\t [-1.2868648  1.2693707] \t1\ttrue\n",
            "(1)\t 239\t [-1.6436765  2.2585266] \t1\ttrue\n",
            "(0)\t 240\t [ 1.7031039 -2.1821928] \t0\ttrue\n",
            "(0)\t 241\t [ 1.6956304 -2.2247028] \t0\ttrue\n",
            "(1)\t 242\t [-1.6967756  2.2258675] \t1\ttrue\n",
            "(1)\t 243\t [ 0.90873426 -1.1589447 ] \t0\tfalse\n",
            "(0)\t 244\t [ 1.9342808 -2.5083227] \t0\ttrue\n",
            "(1)\t 245\t [-1.6916925  2.2687187] \t1\ttrue\n",
            "(0)\t 246\t [-1.0519072  1.0318409] \t1\tfalse\n",
            "(1)\t 247\t [-1.6107236  2.0820067] \t1\ttrue\n",
            "(0)\t 248\t [ 0.93462324 -1.1348615 ] \t0\ttrue\n",
            "(0)\t 249\t [-0.6502115  0.5154787] \t1\tfalse\n",
            "(1)\t 250\t [ 1.6708027 -2.1303086] \t0\tfalse\n",
            "(0)\t 251\t [ 2.1139753 -2.6728578] \t0\ttrue\n",
            "(1)\t 252\t [-1.4046909  2.022127 ] \t1\ttrue\n",
            "(0)\t 253\t [ 0.78185016 -1.083168  ] \t0\ttrue\n",
            "(1)\t 254\t [ 1.4513241 -1.6714729] \t0\tfalse\n",
            "(0)\t 255\t [ 1.9424208 -2.349374 ] \t0\ttrue\n",
            "(1)\t 256\t [-1.4611813  1.7668672] \t1\ttrue\n",
            "(0)\t 257\t [ 1.7626295 -2.3503213] \t0\ttrue\n",
            "(1)\t 258\t [ 1.0696526 -1.340883 ] \t0\tfalse\n",
            "(1)\t 259\t [-1.2716742  1.1994647] \t1\ttrue\n",
            "(0)\t 260\t [ 1.3538648 -1.8189365] \t0\ttrue\n",
            "(1)\t 261\t [-1.7391676  2.1748505] \t1\ttrue\n",
            "(0)\t 262\t [ 1.3210372 -1.8274792] \t0\ttrue\n",
            "(1)\t 263\t [-0.383877    0.09962104] \t1\ttrue\n",
            "(0)\t 264\t [-0.383877    0.09962104] \t1\tfalse\n",
            "(1)\t 265\t [ 0.7720135 -1.011992 ] \t0\tfalse\n",
            "(0)\t 266\t [ 1.8642814 -2.4249768] \t0\ttrue\n",
            "(1)\t 267\t [-1.6598452  2.148174 ] \t1\ttrue\n",
            "(0)\t 268\t [ 1.2167553 -1.6703209] \t0\ttrue\n",
            "(0)\t 269\t [ 1.7309133 -2.3623068] \t0\ttrue\n",
            "(1)\t 270\t [-1.1035348  1.0358223] \t1\ttrue\n",
            "(0)\t 271\t [ 1.8876241 -2.414187 ] \t0\ttrue\n",
            "(1)\t 272\t [-1.4816616  1.8481048] \t1\ttrue\n",
            "(1)\t 273\t [-1.6350561  2.0171552] \t1\ttrue\n",
            "(0)\t 274\t [-0.8177618  0.6633613] \t1\tfalse\n",
            "(0)\t 275\t [ 1.0189613 -1.3915417] \t0\ttrue\n",
            "(1)\t 276\t [-1.646074   2.1697056] \t1\ttrue\n",
            "(0)\t 277\t [-0.29588366  0.24455109] \t1\tfalse\n",
            "(1)\t 278\t [-1.6000388  1.9789966] \t1\ttrue\n",
            "(1)\t 279\t [-0.84149855  0.7022582 ] \t1\ttrue\n",
            "(0)\t 280\t [ 1.9174098 -2.4996436] \t0\ttrue\n",
            "(1)\t 281\t [-1.48178    2.0075073] \t1\ttrue\n",
            "(0)\t 282\t [ 1.8932389 -2.494899 ] \t0\ttrue\n",
            "(0)\t 283\t [ 1.4454967 -1.8902556] \t0\ttrue\n",
            "(1)\t 284\t [-0.53804463  0.23586282] \t1\ttrue\n",
            "(0)\t 285\t [ 1.9978923 -2.592677 ] \t0\ttrue\n",
            "(0)\t 286\t [-1.6070484  1.9410192] \t1\tfalse\n",
            "(1)\t 287\t [-1.7865648  2.196273 ] \t1\ttrue\n",
            "(0)\t 288\t [-0.05693457 -0.02532453] \t1\tfalse\n",
            "(1)\t 289\t [-1.6753528  2.0574212] \t1\ttrue\n",
            "(1)\t 290\t [-1.663711   2.0396879] \t1\ttrue\n",
            "(0)\t 291\t [-0.65192014  0.4930102 ] \t1\tfalse\n",
            "(0)\t 292\t [-0.35502222  0.0859772 ] \t1\tfalse\n",
            "(1)\t 293\t [-0.35502222  0.0859772 ] \t1\ttrue\n",
            "(0)\t 294\t [ 1.8463769 -2.5085764] \t0\ttrue\n",
            "(1)\t 295\t [-1.7932864  2.2390168] \t1\ttrue\n",
            "(0)\t 296\t [ 1.7269487 -2.2231472] \t0\ttrue\n",
            "(0)\t 297\t [ 1.833986  -2.3413482] \t0\ttrue\n",
            "(1)\t 298\t [-1.7677379  2.2352605] \t1\ttrue\n",
            "(0)\t 299\t [ 1.647323 -2.124517] \t0\ttrue\n",
            "(1)\t 300\t [-1.6607485  2.1340933] \t1\ttrue\n",
            "(0)\t 301\t [ 1.7519004 -2.300708 ] \t0\ttrue\n",
            "(1)\t 302\t [-1.4290437  2.0145578] \t1\ttrue\n",
            "(1)\t 303\t [-1.5985419  2.1239405] \t1\ttrue\n",
            "(0)\t 304\t [ 1.7455701 -2.2823167] \t0\ttrue\n",
            "(1)\t 305\t [-1.7285701  2.0373242] \t1\ttrue\n",
            "(0)\t 306\t [ 0.5005668 -0.7419482] \t0\ttrue\n",
            "(0)\t 307\t [ 1.6595255 -1.9350978] \t0\ttrue\n",
            "(1)\t 308\t [-1.6361985  2.234107 ] \t1\ttrue\n",
            "(0)\t 309\t [ 1.8597783 -2.2962866] \t0\ttrue\n",
            "(0)\t 310\t [-0.7748156  0.5374838] \t1\tfalse\n",
            "(1)\t 311\t [-1.6319776  1.9846295] \t1\ttrue\n",
            "(1)\t 312\t [-1.289527  1.237737] \t1\ttrue\n",
            "(0)\t 313\t [ 0.81111574 -1.1249928 ] \t0\ttrue\n",
            "(1)\t 314\t [ 0.8047106 -1.1483774] \t0\tfalse\n",
            "(0)\t 315\t [ 1.6768159 -2.3006506] \t0\ttrue\n",
            "(1)\t 316\t [-1.7013669  2.1330764] \t1\ttrue\n",
            "(0)\t 317\t [ 1.6946198 -2.1494694] \t0\ttrue\n",
            "(0)\t 318\t [ 2.0645933 -2.5432348] \t0\ttrue\n",
            "(1)\t 319\t [-1.6764783  2.127546 ] \t1\ttrue\n",
            "(0)\t 320\t [-0.44220573  0.37244037] \t1\tfalse\n",
            "(1)\t 321\t [-1.5993032  2.0716386] \t1\ttrue\n",
            "(1)\t 322\t [-1.7411127  2.1921196] \t1\ttrue\n",
            "(0)\t 323\t [ 1.9964468 -2.4819317] \t0\ttrue\n",
            "(0)\t 324\t [ 2.074759 -2.575891] \t0\ttrue\n",
            "(1)\t 325\t [-1.6721042  1.9532721] \t1\ttrue\n",
            "(0)\t 326\t [ 1.86905 -2.38166] \t0\ttrue\n",
            "(0)\t 327\t [-0.09082574 -0.27417457] \t0\ttrue\n",
            "(1)\t 328\t [-0.09082574 -0.27417457] \t0\tfalse\n",
            "(1)\t 329\t [-1.4059381  2.0575726] \t1\ttrue\n",
            "(0)\t 330\t [ 1.2715378 -1.5739875] \t0\ttrue\n",
            "(0)\t 331\t [ 1.80693  -2.256748] \t0\ttrue\n",
            "(1)\t 332\t [-1.2530378  1.2208196] \t1\ttrue\n",
            "(0)\t 333\t [-1.4241936  1.6168051] \t1\tfalse\n",
            "(1)\t 334\t [-1.6500466  2.0124831] \t1\ttrue\n",
            "(0)\t 335\t [-0.54092133  0.27037457] \t1\tfalse\n",
            "(1)\t 336\t [-1.68568   2.133211] \t1\ttrue\n",
            "(0)\t 337\t [ 1.7263571 -2.3166182] \t0\ttrue\n",
            "(0)\t 338\t [ 1.7532398 -2.264102 ] \t0\ttrue\n",
            "(0)\t 339\t [ 1.7670048 -2.2915742] \t0\ttrue\n",
            "(1)\t 340\t [-1.600151   2.1773698] \t1\ttrue\n",
            "(1)\t 341\t [-1.5381254  1.8109845] \t1\ttrue\n",
            "(0)\t 342\t [ 1.9483451 -2.4497309] \t0\ttrue\n",
            "(0)\t 343\t [-0.6781959  0.5139381] \t1\tfalse\n",
            "(1)\t 344\t [-1.4688486  1.9854369] \t1\ttrue\n",
            "(1)\t 345\t [-1.7328551  2.3145423] \t1\ttrue\n",
            "(0)\t 346\t [ 1.0743767 -1.4021446] \t0\ttrue\n",
            "(1)\t 347\t [ 0.9245668 -1.2368163] \t0\tfalse\n",
            "(0)\t 348\t [ 2.013698 -2.559788] \t0\ttrue\n",
            "(0)\t 349\t [-0.16119628 -0.10005677] \t1\tfalse\n",
            "(1)\t 350\t [-0.16119628 -0.10005677] \t1\ttrue\n",
            "(1)\t 351\t [ 0.44107965 -0.7795045 ] \t0\tfalse\n",
            "(0)\t 352\t [ 0.44107965 -0.7795045 ] \t0\ttrue\n",
            "(0)\t 353\t [ 1.8099357 -2.3108253] \t0\ttrue\n",
            "(1)\t 354\t [-1.2995927  1.893076 ] \t1\ttrue\n",
            "(1)\t 355\t [-1.3686814  1.3947158] \t1\ttrue\n",
            "(0)\t 356\t [ 1.9853044 -2.6143312] \t0\ttrue\n",
            "(0)\t 357\t [ 1.7125565 -2.0115361] \t0\ttrue\n",
            "(1)\t 358\t [-1.6698273  2.1727018] \t1\ttrue\n",
            "(1)\t 359\t [-1.3377459  1.3406682] \t1\ttrue\n",
            "(0)\t 360\t [ 1.8393265 -2.4384384] \t0\ttrue\n",
            "(1)\t 361\t [-1.6155554  2.135044 ] \t1\ttrue\n",
            "(0)\t 362\t [ 1.8627514 -2.4194894] \t0\ttrue\n",
            "(1)\t 363\t [-1.5998532  2.0496714] \t1\ttrue\n",
            "(0)\t 364\t [ 1.7878113 -2.3343332] \t0\ttrue\n",
            "(1)\t 365\t [-1.7178001  2.1429312] \t1\ttrue\n",
            "(0)\t 366\t [ 1.8112302 -2.3427322] \t0\ttrue\n",
            "(0)\t 367\t [-0.9072309   0.74352986] \t1\tfalse\n",
            "(1)\t 368\t [-1.2243826  1.2309864] \t1\ttrue\n",
            "(1)\t 369\t [-1.66211   2.054513] \t1\ttrue\n",
            "(0)\t 370\t [ 1.9849277 -2.6022086] \t0\ttrue\n",
            "(0)\t 371\t [ 1.9461554 -2.5284634] \t0\ttrue\n",
            "(1)\t 372\t [-1.5804662  1.7511084] \t1\ttrue\n",
            "(1)\t 373\t [-1.6824343  2.2593791] \t1\ttrue\n",
            "(0)\t 374\t [ 1.0864022 -1.4301602] \t0\ttrue\n",
            "(0)\t 375\t [ 1.1533242 -1.5294372] \t0\ttrue\n",
            "(1)\t 376\t [-1.6887826  2.1450443] \t1\ttrue\n",
            "(1)\t 377\t [-0.8192925  0.6712122] \t1\ttrue\n",
            "(0)\t 378\t [ 2.0105865 -2.5621066] \t0\ttrue\n",
            "(1)\t 379\t [ 1.7928367 -2.170569 ] \t0\tfalse\n",
            "(0)\t 380\t [ 1.6686672 -2.0872512] \t0\ttrue\n",
            "(0)\t 381\t [ 2.0543313 -2.493625 ] \t0\ttrue\n",
            "(1)\t 382\t [ 0.01203004 -0.2914171 ] \t0\tfalse\n",
            "(0)\t 383\t [ 0.01203004 -0.2914171 ] \t0\ttrue\n",
            "(0)\t 384\t [ 1.5362158 -1.8807228] \t0\ttrue\n",
            "(0)\t 385\t [ 1.7814965 -2.3460023] \t0\ttrue\n",
            "(0)\t 386\t [ 2.1047497 -2.512807 ] \t0\ttrue\n",
            "(1)\t 387\t [-1.471631  1.97058 ] \t1\ttrue\n",
            "(0)\t 388\t [ 1.6798822 -2.23343  ] \t0\ttrue\n",
            "(1)\t 389\t [-0.17501639 -0.12798059] \t1\ttrue\n",
            "(0)\t 390\t [-0.17501639 -0.12798059] \t1\tfalse\n",
            "(0)\t 391\t [ 1.4195405 -1.9036908] \t0\ttrue\n",
            "(0)\t 392\t [-0.22103088 -0.01428083] \t1\tfalse\n",
            "(1)\t 393\t [-1.5940734  2.0968509] \t1\ttrue\n",
            "(0)\t 394\t [-0.01498085 -0.24657045] \t0\ttrue\n",
            "(1)\t 395\t [ 1.4677745 -1.8034298] \t0\tfalse\n",
            "(0)\t 396\t [ 1.9078829 -2.489827 ] \t0\ttrue\n",
            "(1)\t 397\t [-1.390278   1.9775513] \t1\ttrue\n",
            "(0)\t 398\t [-1.4565548  1.7985924] \t1\tfalse\n",
            "(1)\t 399\t [-1.0805744  1.1028752] \t1\ttrue\n",
            "(0)\t 400\t [ 1.8307095 -2.4176176] \t0\ttrue\n",
            "(0)\t 401\t [ 1.8641592 -2.406028 ] \t0\ttrue\n",
            "(1)\t 402\t [-1.6278139  2.0581906] \t1\ttrue\n",
            "(1)\t 403\t [ 1.0661368 -1.2961224] \t0\tfalse\n",
            "(0)\t 404\t [ 0.76137954 -1.0465972 ] \t0\ttrue\n",
            "(0)\t 405\t [ 0.6103867 -0.8638761] \t0\ttrue\n",
            "(1)\t 406\t [-1.6564548  2.2450297] \t1\ttrue\n",
            "(1)\t 407\t [-1.5947951  2.2224224] \t1\ttrue\n",
            "(0)\t 408\t [ 1.6213673 -2.2226367] \t0\ttrue\n",
            "(0)\t 409\t [ 1.1468191 -1.6627525] \t0\ttrue\n",
            "(0)\t 410\t [ 1.9036654 -2.4773128] \t0\ttrue\n",
            "(1)\t 411\t [-1.6508112  2.2327638] \t1\ttrue\n",
            "(0)\t 412\t [ 1.836104  -2.4217641] \t0\ttrue\n",
            "(0)\t 413\t [ 1.6009183 -2.0607743] \t0\ttrue\n",
            "(1)\t 414\t [-1.7165314  2.2314973] \t1\ttrue\n",
            "(1)\t 415\t [-1.6155154  2.2022598] \t1\ttrue\n",
            "(0)\t 416\t [ 1.1973417 -1.6185367] \t0\ttrue\n",
            "(1)\t 417\t [-1.681881   2.1425345] \t1\ttrue\n",
            "(0)\t 418\t [ 1.8958614 -2.37115  ] \t0\ttrue\n",
            "(0)\t 419\t [ 1.7914994 -2.2335129] \t0\ttrue\n",
            "(1)\t 420\t [-1.1738728  1.4068825] \t1\ttrue\n",
            "(1)\t 421\t [-1.5660218  1.9192569] \t1\ttrue\n",
            "(0)\t 422\t [-0.33113575  0.13613655] \t1\tfalse\n",
            "(1)\t 423\t [-1.1364194  1.1568667] \t1\ttrue\n",
            "(0)\t 424\t [-0.10681301 -0.21104182] \t0\ttrue\n",
            "(0)\t 425\t [ 0.7286736 -0.9543261] \t0\ttrue\n",
            "(1)\t 426\t [-1.1082839  1.2015433] \t1\ttrue\n",
            "(0)\t 427\t [-0.16484699 -0.0431219 ] \t1\tfalse\n",
            "(1)\t 428\t [-1.5855429  2.0510428] \t1\ttrue\n",
            "(0)\t 429\t [ 1.6605171 -2.163347 ] \t0\ttrue\n",
            "(1)\t 430\t [-0.4378729   0.18089505] \t1\ttrue\n",
            "(0)\t 431\t [ 1.3495859 -1.6662303] \t0\ttrue\n",
            "(0)\t 432\t [ 1.0942947 -1.5552579] \t0\ttrue\n",
            "(1)\t 433\t [-1.1629587  1.1460063] \t1\ttrue\n",
            "(1)\t 434\t [-0.6355985   0.53196996] \t1\ttrue\n",
            "(0)\t 435\t [ 1.0812858 -1.5003213] \t0\ttrue\n",
            "(0)\t 436\t [ 1.8500121 -2.4360685] \t0\ttrue\n",
            "(0)\t 437\t [ 1.3906258 -1.8566152] \t0\ttrue\n",
            "(1)\t 438\t [-0.6660993   0.52558625] \t1\ttrue\n",
            "(0)\t 439\t [-1.3834839  1.938321 ] \t1\tfalse\n",
            "(1)\t 440\t [-1.6935552  2.2101607] \t1\ttrue\n",
            "(1)\t 441\t [-1.59749    2.1224093] \t1\ttrue\n",
            "(0)\t 442\t [ 1.8522934 -2.460954 ] \t0\ttrue\n",
            "(0)\t 443\t [ 1.8945671 -2.4657578] \t0\ttrue\n",
            "(1)\t 444\t [-1.6255687  2.0831847] \t1\ttrue\n",
            "(0)\t 445\t [ 0.05760156 -0.41761923] \t0\ttrue\n",
            "(0)\t 446\t [ 1.7934897 -2.3359826] \t0\ttrue\n",
            "(0)\t 447\t [ 1.7939664 -2.1533248] \t0\ttrue\n",
            "(0)\t 448\t [ 1.3166517 -1.5565823] \t0\ttrue\n",
            "(0)\t 449\t [ 1.8141943 -2.3307564] \t0\ttrue\n",
            "(1)\t 450\t [-1.5591528  2.1006157] \t1\ttrue\n",
            "(1)\t 451\t [-0.06967885 -0.19196267] \t0\tfalse\n",
            "(0)\t 452\t [-0.06967885 -0.19196267] \t0\ttrue\n",
            "(1)\t 453\t [-1.7205801  2.2173095] \t1\ttrue\n",
            "(0)\t 454\t [ 1.1134381 -1.4881709] \t0\ttrue\n",
            "(0)\t 455\t [ 1.1923907 -1.6052202] \t0\ttrue\n",
            "(0)\t 456\t [ 1.9975713 -2.4366906] \t0\ttrue\n",
            "(1)\t 457\t [-1.5004731  2.0542603] \t1\ttrue\n",
            "(1)\t 458\t [-1.3219869  1.8688782] \t1\ttrue\n",
            "(0)\t 459\t [ 1.9734832 -2.5096152] \t0\ttrue\n",
            "(0)\t 460\t [ 1.284109  -1.5280061] \t0\ttrue\n",
            "(1)\t 461\t [-1.3369943  1.2693504] \t1\ttrue\n",
            "(0)\t 462\t [ 2.1486735 -2.5311997] \t0\ttrue\n",
            "(0)\t 463\t [ 1.9339678 -2.4112067] \t0\ttrue\n",
            "(1)\t 464\t [-1.6654744  2.094584 ] \t1\ttrue\n",
            "(0)\t 465\t [ 2.004959  -2.5103338] \t0\ttrue\n",
            "(1)\t 466\t [-1.5931959  2.1069078] \t1\ttrue\n",
            "(0)\t 467\t [ 1.7202498 -2.1773734] \t0\ttrue\n",
            "(0)\t 468\t [ 0.98761934 -1.3324634 ] \t0\ttrue\n",
            "(0)\t 469\t [ 1.7588284 -2.2331808] \t0\ttrue\n",
            "(0)\t 470\t [-0.40488797  0.132791  ] \t1\tfalse\n",
            "(1)\t 471\t [-1.6019267  2.2456567] \t1\ttrue\n",
            "(0)\t 472\t [ 2.022992  -2.5261226] \t0\ttrue\n",
            "(1)\t 473\t [-1.5873624  1.8665547] \t1\ttrue\n",
            "(0)\t 474\t [ 1.561994  -2.2244563] \t0\ttrue\n",
            "(1)\t 475\t [-1.009922    0.97064024] \t1\ttrue\n",
            "(0)\t 476\t [ 1.6619955 -2.2340207] \t0\ttrue\n",
            "(0)\t 477\t [ 1.4399339 -2.0761042] \t0\ttrue\n",
            "(1)\t 478\t [-0.76196265  0.5392781 ] \t1\ttrue\n",
            "(0)\t 479\t [ 1.6553432 -2.178803 ] \t0\ttrue\n",
            "(1)\t 480\t [-1.7233061  2.2551742] \t1\ttrue\n",
            "(1)\t 481\t [-1.4597123  1.816345 ] \t1\ttrue\n",
            "(0)\t 482\t [-1.186452   1.2563035] \t1\tfalse\n",
            "(1)\t 483\t [-1.395824  1.929963] \t1\ttrue\n",
            "(0)\t 484\t [ 1.5905511 -1.9652513] \t0\ttrue\n",
            "(1)\t 485\t [-1.4122291  2.0258043] \t1\ttrue\n",
            "(0)\t 486\t [ 2.0708761 -2.5925734] \t0\ttrue\n",
            "(0)\t 487\t [ 2.1971536 -2.5248065] \t0\ttrue\n",
            "(1)\t 488\t [-1.6995584  2.3095465] \t1\ttrue\n",
            "(0)\t 489\t [ 0.0632624 -0.3935754] \t0\ttrue\n",
            "(1)\t 490\t [-0.35415888  0.12196141] \t1\ttrue\n",
            "(1)\t 491\t [-1.5796094  1.9013406] \t1\ttrue\n",
            "(0)\t 492\t [-0.6219771  0.3359011] \t1\tfalse\n",
            "(0)\t 493\t [ 1.9476118 -2.3514307] \t0\ttrue\n",
            "(1)\t 494\t [-1.5812469  2.02028  ] \t1\ttrue\n",
            "(0)\t 495\t [ 1.9693547 -2.1848025] \t0\ttrue\n",
            "(0)\t 496\t [-0.78551555  0.59672767] \t1\tfalse\n",
            "(1)\t 497\t [-1.0737609  1.0191733] \t1\ttrue\n",
            "(1)\t 498\t [-1.4684398  2.0516694] \t1\ttrue\n",
            "(0)\t 499\t [ 1.6683297 -2.1856263] \t0\ttrue\n",
            "(1)\t 500\t [-1.6140505  2.1689866] \t1\ttrue\n",
            "(0)\t 501\t [ 1.557861  -2.0781767] \t0\ttrue\n",
            "(1)\t 502\t [-1.6511104  2.1271832] \t1\ttrue\n",
            "(0)\t 503\t [ 1.8716806 -2.4980447] \t0\ttrue\n",
            "(0)\t 504\t [-0.22999857 -0.08748178] \t1\tfalse\n",
            "(1)\t 505\t [-0.22999857 -0.08748178] \t1\ttrue\n",
            "(0)\t 506\t [ 1.8509147 -2.4368744] \t0\ttrue\n",
            "(0)\t 507\t [ 0.86609143 -1.2584229 ] \t0\ttrue\n",
            "(1)\t 508\t [-1.6789066  2.09076  ] \t1\ttrue\n",
            "(0)\t 509\t [ 1.5724818 -1.9735098] \t0\ttrue\n",
            "(0)\t 510\t [ 1.8844991 -2.444811 ] \t0\ttrue\n",
            "(0)\t 511\t [ 1.9886738 -2.5407746] \t0\ttrue\n",
            "(0)\t 512\t [ 1.6531597 -2.212226 ] \t0\ttrue\n",
            "(1)\t 513\t [ 1.4757088 -1.9931811] \t0\tfalse\n",
            "(0)\t 514\t [-0.42213386  0.15302739] \t1\tfalse\n",
            "(0)\t 515\t [ 1.7458979 -2.0168488] \t0\ttrue\n",
            "(0)\t 516\t [ 2.0410414 -2.5197906] \t0\ttrue\n",
            "(1)\t 517\t [-1.7634112  2.2428324] \t1\ttrue\n",
            "(1)\t 518\t [-1.7547133  2.3037055] \t1\ttrue\n",
            "(0)\t 519\t [ 1.4115242 -1.899242 ] \t0\ttrue\n",
            "(0)\t 520\t [-1.6394702  1.9543415] \t1\tfalse\n",
            "(1)\t 521\t [-1.6685008  2.2191865] \t1\ttrue\n",
            "(0)\t 522\t [ 1.2776223 -1.6799217] \t0\ttrue\n",
            "(1)\t 523\t [-0.4631229   0.24868087] \t1\ttrue\n",
            "(0)\t 524\t [ 1.7717733 -2.2727766] \t0\ttrue\n",
            "(1)\t 525\t [-1.5773574  2.2100105] \t1\ttrue\n",
            "(0)\t 526\t [ 1.7228546 -2.2840495] \t0\ttrue\n",
            "(0)\t 527\t [-0.59164315  0.8478929 ] \t1\tfalse\n",
            "(1)\t 528\t [-1.596755  2.140369] \t1\ttrue\n",
            "(0)\t 529\t [ 0.4046763 -0.7158268] \t0\ttrue\n",
            "(1)\t 530\t [-1.7149572  2.1201043] \t1\ttrue\n",
            "(0)\t 531\t [-1.0006366  0.8737541] \t1\tfalse\n",
            "(1)\t 532\t [-1.240948   1.2943294] \t1\ttrue\n",
            "(0)\t 533\t [ 1.9213268 -2.3978956] \t0\ttrue\n",
            "(1)\t 534\t [-1.6624401  1.9414382] \t1\ttrue\n",
            "(0)\t 535\t [ 1.989277  -2.5465248] \t0\ttrue\n",
            "(0)\t 536\t [ 2.0025039 -2.5890284] \t0\ttrue\n",
            "(1)\t 537\t [-1.3197037  1.9241576] \t1\ttrue\n",
            "(1)\t 538\t [-0.6030114   0.37927255] \t1\ttrue\n",
            "(0)\t 539\t [ 1.665639  -2.0541577] \t0\ttrue\n",
            "(0)\t 540\t [ 1.814927 -2.320089] \t0\ttrue\n",
            "(1)\t 541\t [-1.5664403  2.144181 ] \t1\ttrue\n",
            "(0)\t 542\t [ 1.8196577 -2.466813 ] \t0\ttrue\n",
            "(1)\t 543\t [-1.1329194  1.7274667] \t1\ttrue\n",
            "(0)\t 544\t [ 1.9657264 -2.4429882] \t0\ttrue\n",
            "(0)\t 545\t [-0.01638081 -0.34622607] \t0\ttrue\n",
            "(0)\t 546\t [-0.8697676   0.84008485] \t1\tfalse\n",
            "(1)\t 547\t [-1.721053   2.3054702] \t1\ttrue\n",
            "(1)\t 548\t [ 0.5684412 -0.9344565] \t0\tfalse\n",
            "(0)\t 549\t [ 0.5684412 -0.9344565] \t0\ttrue\n",
            "(0)\t 550\t [ 1.4643086 -1.9493695] \t0\ttrue\n",
            "(1)\t 551\t [-1.7139634  2.138039 ] \t1\ttrue\n",
            "(0)\t 552\t [ 1.4197472 -1.9562504] \t0\ttrue\n",
            "(1)\t 553\t [-0.66918606  0.49028075] \t1\ttrue\n",
            "(1)\t 554\t [-1.7351032  2.1117356] \t1\ttrue\n",
            "(0)\t 555\t [ 1.908356  -2.5321114] \t0\ttrue\n",
            "(0)\t 556\t [ 1.8199551 -2.0837522] \t0\ttrue\n",
            "(1)\t 557\t [-1.6205608  2.172977 ] \t1\ttrue\n",
            "(0)\t 558\t [ 1.9142447 -2.470066 ] \t0\ttrue\n",
            "(0)\t 559\t [ 1.9552518 -2.4520233] \t0\ttrue\n",
            "(1)\t 560\t [-1.692896   2.2667487] \t1\ttrue\n",
            "(1)\t 561\t [-1.5175897  1.8145317] \t1\ttrue\n",
            "(0)\t 562\t [-0.77945673  0.6057921 ] \t1\tfalse\n",
            "(0)\t 563\t [-0.00780772 -0.33876723] \t0\ttrue\n",
            "(1)\t 564\t [-0.00780772 -0.33876723] \t0\tfalse\n",
            "(0)\t 565\t [ 1.6816674 -2.275589 ] \t0\ttrue\n",
            "(1)\t 566\t [-1.6263498  1.9890517] \t1\ttrue\n",
            "(1)\t 567\t [-1.6192153  2.1490374] \t1\ttrue\n",
            "(0)\t 568\t [ 1.8917114 -2.5169368] \t0\ttrue\n",
            "(0)\t 569\t [ 2.0779834 -2.5654175] \t0\ttrue\n",
            "(1)\t 570\t [-1.7520124  2.2474833] \t1\ttrue\n",
            "(0)\t 571\t [ 1.2081293 -1.668893 ] \t0\ttrue\n",
            "(1)\t 572\t [-1.5799791  1.8315322] \t1\ttrue\n",
            "(0)\t 573\t [ 1.883435  -2.4625137] \t0\ttrue\n",
            "(1)\t 574\t [-1.7138416  2.2727766] \t1\ttrue\n",
            "(1)\t 575\t [-1.222826   1.8207916] \t1\ttrue\n",
            "(0)\t 576\t [ 1.9247166 -2.286646 ] \t0\ttrue\n",
            "(1)\t 577\t [-1.5921794  1.7806562] \t1\ttrue\n",
            "(0)\t 578\t [-1.5921794  1.7806562] \t1\tfalse\n",
            "(1)\t 579\t [-1.2074832  1.2872549] \t1\ttrue\n",
            "(0)\t 580\t [ 1.7611237 -2.2283292] \t0\ttrue\n",
            "(0)\t 581\t [-0.0861192  -0.20861517] \t0\ttrue\n",
            "(1)\t 582\t [-0.0861192  -0.20861517] \t0\tfalse\n",
            "(0)\t 583\t [ 0.07900093 -0.42226356] \t0\ttrue\n",
            "(1)\t 584\t [ 0.07900093 -0.42226356] \t0\tfalse\n",
            "(1)\t 585\t [-1.6125546  2.176607 ] \t1\ttrue\n",
            "(0)\t 586\t [ 1.5458926 -2.0629036] \t0\ttrue\n",
            "(1)\t 587\t [-1.5512235  2.1081371] \t1\ttrue\n",
            "(0)\t 588\t [-0.989      0.9741849] \t1\tfalse\n",
            "(1)\t 589\t [-1.7345631  2.190208 ] \t1\ttrue\n",
            "(0)\t 590\t [-1.7345631  2.190208 ] \t1\tfalse\n",
            "(0)\t 591\t [-1.277566   1.4748672] \t1\tfalse\n",
            "(1)\t 592\t [-1.6431096  1.9484223] \t1\ttrue\n",
            "(0)\t 593\t [ 2.0255654 -2.563414 ] \t0\ttrue\n",
            "(1)\t 594\t [-1.6149367  1.9160893] \t1\ttrue\n",
            "(1)\t 595\t [-1.5084494  1.7600685] \t1\ttrue\n",
            "(0)\t 596\t [-0.565502    0.34157315] \t1\tfalse\n",
            "(1)\t 597\t [-1.1355766  1.1909213] \t1\ttrue\n",
            "(0)\t 598\t [ 1.7289454 -2.1801865] \t0\ttrue\n",
            "(1)\t 599\t [-1.4565802  1.5404186] \t1\ttrue\n",
            "(0)\t 600\t [ 1.6678177 -2.2450643] \t0\ttrue\n",
            "(0)\t 601\t [ 1.015477  -1.3215137] \t0\ttrue\n",
            "(1)\t 602\t [-1.7171292  2.294887 ] \t1\ttrue\n",
            "(0)\t 603\t [ 1.4765244 -1.8540527] \t0\ttrue\n",
            "(1)\t 604\t [-1.0640907  0.9857353] \t1\ttrue\n",
            "(1)\t 605\t [-1.494692   2.0368335] \t1\ttrue\n",
            "(0)\t 606\t [ 1.959196  -2.2238648] \t0\ttrue\n",
            "(1)\t 607\t [-1.2492862  1.3181986] \t1\ttrue\n",
            "(0)\t 608\t [ 1.4060858 -1.8075626] \t0\ttrue\n",
            "(1)\t 609\t [-1.4467081  2.0910494] \t1\ttrue\n",
            "(0)\t 610\t [ 1.8897156 -2.39297  ] \t0\ttrue\n",
            "(0)\t 611\t [ 1.7166193 -2.2512355] \t0\ttrue\n",
            "(0)\t 612\t [-0.23300476 -0.1345499 ] \t1\tfalse\n",
            "(0)\t 613\t [ 1.849474  -2.3756537] \t0\ttrue\n",
            "(1)\t 614\t [-1.7103461  2.262056 ] \t1\ttrue\n",
            "(0)\t 615\t [ 1.861019 -2.487111] \t0\ttrue\n",
            "(1)\t 616\t [-0.87208986  0.77739656] \t1\ttrue\n",
            "(0)\t 617\t [ 1.1526958 -1.601011 ] \t0\ttrue\n",
            "(0)\t 618\t [ 1.9410746 -2.4064412] \t0\ttrue\n",
            "(0)\t 619\t [-0.24303322 -0.06425077] \t1\tfalse\n",
            "(1)\t 620\t [-1.4574016  1.9269601] \t1\ttrue\n",
            "(0)\t 621\t [ 1.951907  -2.4159498] \t0\ttrue\n",
            "(1)\t 622\t [-1.6797839  2.137962 ] \t1\ttrue\n",
            "(1)\t 623\t [-1.7371933  2.209624 ] \t1\ttrue\n",
            "(0)\t 624\t [ 1.1247228 -1.4150311] \t0\ttrue\n",
            "(0)\t 625\t [-0.6088234   0.33909237] \t1\tfalse\n",
            "(0)\t 626\t [ 2.119924  -2.6470683] \t0\ttrue\n",
            "(1)\t 627\t [-1.590855   2.1573215] \t1\ttrue\n",
            "(1)\t 628\t [-1.4218231  1.5302728] \t1\ttrue\n",
            "(0)\t 629\t [ 1.853976  -2.3982456] \t0\ttrue\n",
            "(1)\t 630\t [-1.7427877  2.2769244] \t1\ttrue\n",
            "(0)\t 631\t [ 1.7373139 -2.2291563] \t0\ttrue\n",
            "(0)\t 632\t [ 1.9646307 -2.5354044] \t0\ttrue\n",
            "(1)\t 633\t [-0.8988876  0.726037 ] \t1\ttrue\n",
            "(0)\t 634\t [ 0.88518804 -1.0595015 ] \t0\ttrue\n",
            "(1)\t 635\t [ 0.77315074 -1.0720304 ] \t0\tfalse\n",
            "(0)\t 636\t [ 1.7132392 -2.304295 ] \t0\ttrue\n",
            "(1)\t 637\t [-1.4857522  1.6591183] \t1\ttrue\n",
            "(1)\t 638\t [-1.6513646  2.1537418] \t1\ttrue\n",
            "(0)\t 639\t [ 1.9806134 -2.5636704] \t0\ttrue\n",
            "(0)\t 640\t [ 1.4726189 -2.0753791] \t0\ttrue\n",
            "(1)\t 641\t [-1.2801374  1.309435 ] \t1\ttrue\n",
            "(0)\t 642\t [ 0.10359611 -0.41121277] \t0\ttrue\n",
            "(1)\t 643\t [ 0.10359611 -0.41121277] \t0\tfalse\n",
            "(1)\t 644\t [-1.4962417  1.8566195] \t1\ttrue\n",
            "(0)\t 645\t [ 1.5407643 -2.0406682] \t0\ttrue\n",
            "(0)\t 646\t [-0.08141097 -0.21393971] \t0\ttrue\n",
            "(1)\t 647\t [-0.64266235  0.5868604 ] \t1\ttrue\n",
            "(0)\t 648\t [-0.2842737   0.02839185] \t1\tfalse\n",
            "(1)\t 649\t [-1.6871091  2.181272 ] \t1\ttrue\n",
            "(1)\t 650\t [-1.5871512  1.902707 ] \t1\ttrue\n",
            "(0)\t 651\t [ 1.951399  -2.4992728] \t0\ttrue\n",
            "(0)\t 652\t [ 1.312759 -1.801036] \t0\ttrue\n",
            "(1)\t 653\t [-1.4660059  1.5945777] \t1\ttrue\n",
            "(1)\t 654\t [-1.2625928  1.8827916] \t1\ttrue\n",
            "(0)\t 655\t [ 2.048591  -2.5183375] \t0\ttrue\n",
            "(1)\t 656\t [-1.5336415  2.1029155] \t1\ttrue\n",
            "(0)\t 657\t [ 1.1492974 -1.5965577] \t0\ttrue\n",
            "(1)\t 658\t [ 1.9338453 -2.4496338] \t0\tfalse\n",
            "(0)\t 659\t [ 1.9997963 -2.3397589] \t0\ttrue\n",
            "(1)\t 660\t [ 1.5578067 -2.0756116] \t0\tfalse\n",
            "(0)\t 661\t [ 1.5308683 -2.1504571] \t0\ttrue\n",
            "(0)\t 662\t [ 2.012545  -2.3976324] \t0\ttrue\n",
            "(1)\t 663\t [ 0.34290075 -0.7276024 ] \t0\tfalse\n",
            "(1)\t 664\t [-1.5267099  2.0543149] \t1\ttrue\n",
            "(0)\t 665\t [-0.7949216  0.5900232] \t1\tfalse\n",
            "(0)\t 666\t [ 0.8264147 -1.0166185] \t0\ttrue\n",
            "(1)\t 667\t [-1.6984202  2.161903 ] \t1\ttrue\n",
            "(1)\t 668\t [-1.6578274  2.1132238] \t1\ttrue\n",
            "(0)\t 669\t [ 1.5208411 -1.8910891] \t0\ttrue\n",
            "(0)\t 670\t [ 1.7804831 -2.3086784] \t0\ttrue\n",
            "(0)\t 671\t [ 1.6432841 -2.1395974] \t0\ttrue\n",
            "(1)\t 672\t [-1.5676061  2.0347445] \t1\ttrue\n",
            "(0)\t 673\t [-0.99626523  0.8185603 ] \t1\tfalse\n",
            "(1)\t 674\t [-1.542968   2.0274327] \t1\ttrue\n",
            "(0)\t 675\t [ 1.7326714 -2.4201088] \t0\ttrue\n",
            "(1)\t 676\t [-1.7804059  2.3501544] \t1\ttrue\n",
            "(0)\t 677\t [-0.3486857   0.05142877] \t1\tfalse\n",
            "(0)\t 678\t [ 1.8009411 -2.35647  ] \t0\ttrue\n",
            "(0)\t 679\t [ 1.6423355 -2.2477689] \t0\ttrue\n",
            "(1)\t 680\t [-1.6682897  2.1273942] \t1\ttrue\n",
            "(0)\t 681\t [ 1.6548898 -2.2379208] \t0\ttrue\n",
            "(1)\t 682\t [-1.306685   1.8538151] \t1\ttrue\n",
            "(0)\t 683\t [ 1.624361  -1.8065152] \t0\ttrue\n",
            "(0)\t 684\t [ 1.6678635 -2.1941533] \t0\ttrue\n",
            "(1)\t 685\t [-1.2301779  1.2212032] \t1\ttrue\n",
            "(0)\t 686\t [ 1.7825903 -2.318582 ] \t0\ttrue\n",
            "(1)\t 687\t [-1.1230986  1.1395478] \t1\ttrue\n",
            "(0)\t 688\t [ 0.35580727 -0.45916274] \t0\ttrue\n",
            "(0)\t 689\t [ 1.0167494 -1.406174 ] \t0\ttrue\n",
            "(1)\t 690\t [-1.2913415  1.3179214] \t1\ttrue\n",
            "(1)\t 691\t [-0.7916701   0.72307575] \t1\ttrue\n",
            "(0)\t 692\t [ 1.5007416 -1.9371892] \t0\ttrue\n",
            "(1)\t 693\t [-1.2570717  1.9071654] \t1\ttrue\n",
            "(0)\t 694\t [ 1.8343563 -2.3061767] \t0\ttrue\n",
            "(0)\t 695\t [ 1.2935182 -1.8754574] \t0\ttrue\n",
            "(1)\t 696\t [-1.6446134  2.115486 ] \t1\ttrue\n",
            "(1)\t 697\t [-0.89511424  0.7051394 ] \t1\ttrue\n",
            "(0)\t 698\t [ 1.8307443 -2.342723 ] \t0\ttrue\n",
            "(0)\t 699\t [ 1.509909  -1.9329257] \t0\ttrue\n",
            "(0)\t 700\t [-1.4726418  1.5571272] \t1\tfalse\n",
            "(1)\t 701\t [ 1.4404048 -1.9320892] \t0\tfalse\n",
            "(0)\t 702\t [ 1.7962586 -2.4017267] \t0\ttrue\n",
            "(1)\t 703\t [-1.7220778  2.209044 ] \t1\ttrue\n",
            "(0)\t 704\t [ 1.4370085 -1.9308134] \t0\ttrue\n",
            "(0)\t 705\t [ 1.883253  -2.3623328] \t0\ttrue\n",
            "(1)\t 706\t [-1.5791532  1.8152064] \t1\ttrue\n",
            "(0)\t 707\t [ 1.8426203 -2.4162867] \t0\ttrue\n",
            "(1)\t 708\t [-1.6593581  2.3028553] \t1\ttrue\n",
            "(1)\t 709\t [-1.5055425  1.7182485] \t1\ttrue\n",
            "(0)\t 710\t [ 1.7945616 -2.4306822] \t0\ttrue\n",
            "(1)\t 711\t [-1.6945144  2.1824   ] \t1\ttrue\n",
            "(0)\t 712\t [ 1.6952218 -2.1913464] \t0\ttrue\n",
            "(0)\t 713\t [-0.65656155  0.50322336] \t1\tfalse\n",
            "(1)\t 714\t [-1.3129611  1.3255713] \t1\ttrue\n",
            "(0)\t 715\t [ 1.7345451 -2.1079543] \t0\ttrue\n",
            "(1)\t 716\t [-1.4873208  2.1703525] \t1\ttrue\n",
            "(1)\t 717\t [-0.8832532  0.8432978] \t1\ttrue\n",
            "(0)\t 718\t [ 1.0848738 -1.4227961] \t0\ttrue\n",
            "(0)\t 719\t [ 1.6764988 -2.2471337] \t0\ttrue\n",
            "(1)\t 720\t [-1.4961522  1.7359895] \t1\ttrue\n",
            "(0)\t 721\t [-1.2700498  1.4224213] \t1\tfalse\n",
            "(0)\t 722\t [ 1.3838431 -1.9796456] \t0\ttrue\n",
            "(1)\t 723\t [-1.6390224  2.1118612] \t1\ttrue\n",
            "(0)\t 724\t [ 0.9976236 -1.3213385] \t0\ttrue\n",
            "(1)\t 725\t [-1.3622402  1.4897174] \t1\ttrue\n",
            "(0)\t 726\t [ 1.9084562 -2.5016696] \t0\ttrue\n",
            "(1)\t 727\t [-1.2209741  1.2405655] \t1\ttrue\n",
            "(0)\t 728\t [-1.2209741  1.2405655] \t1\tfalse\n",
            "(0)\t 729\t [-1.6029375  2.0155838] \t1\tfalse\n",
            "(1)\t 730\t [-1.074152   1.0538679] \t1\ttrue\n",
            "(0)\t 731\t [ 1.6184609 -2.0843182] \t0\ttrue\n",
            "(1)\t 732\t [-1.5995917  2.061359 ] \t1\ttrue\n",
            "(1)\t 733\t [-1.6632094  2.2303944] \t1\ttrue\n",
            "(0)\t 734\t [-1.6213493  2.0753617] \t1\tfalse\n",
            "(0)\t 735\t [ 1.8391873 -2.3205893] \t0\ttrue\n",
            "(0)\t 736\t [ 1.5041834 -2.0362842] \t0\ttrue\n",
            "(1)\t 737\t [-1.7244517  2.0558057] \t1\ttrue\n",
            "(0)\t 738\t [ 1.969966  -2.4944093] \t0\ttrue\n",
            "(0)\t 739\t [ 1.1502178 -1.5380021] \t0\ttrue\n",
            "(1)\t 740\t [-0.90021306  0.7984329 ] \t1\ttrue\n",
            "(0)\t 741\t [-1.7060032  1.9641193] \t1\tfalse\n",
            "(1)\t 742\t [ 0.8101992 -1.07171  ] \t0\tfalse\n",
            "(0)\t 743\t [ 1.6441419 -2.0487254] \t0\ttrue\n",
            "(1)\t 744\t [-1.5250846  2.1208453] \t1\ttrue\n",
            "(1)\t 745\t [-1.509842   1.8929534] \t1\ttrue\n",
            "(0)\t 746\t [-0.71310097  0.52195734] \t1\tfalse\n",
            "(0)\t 747\t [ 1.7251327 -2.3565264] \t0\ttrue\n",
            "(1)\t 748\t [-1.7467415  2.2153478] \t1\ttrue\n",
            "(1)\t 749\t [-1.6042447  2.1175117] \t1\ttrue\n",
            "(0)\t 750\t [-1.1812326  1.3085772] \t1\tfalse\n",
            "(1)\t 751\t [-1.6280034  2.1726077] \t1\ttrue\n",
            "(0)\t 752\t [ 1.814282 -2.353474] \t0\ttrue\n",
            "(0)\t 753\t [ 0.9214801 -1.2756289] \t0\ttrue\n",
            "(0)\t 754\t [ 1.9322999 -2.4922116] \t0\ttrue\n",
            "(1)\t 755\t [-1.7419463  2.2186048] \t1\ttrue\n",
            "(0)\t 756\t [-1.316363   1.2833823] \t1\tfalse\n",
            "(1)\t 757\t [-1.8014333  2.0747228] \t1\ttrue\n",
            "(0)\t 758\t [ 1.9334953 -2.3958406] \t0\ttrue\n",
            "(0)\t 759\t [ 1.1748865 -1.4472438] \t0\ttrue\n",
            "(0)\t 760\t [ 1.8412422 -2.1558154] \t0\ttrue\n",
            "(1)\t 761\t [-1.4795389  1.7471339] \t1\ttrue\n",
            "(0)\t 762\t [ 2.0821311 -2.6363652] \t0\ttrue\n",
            "(1)\t 763\t [-1.4854584  1.9981493] \t1\ttrue\n",
            "(0)\t 764\t [ 1.526878  -2.0236151] \t0\ttrue\n",
            "(0)\t 765\t [-0.00248042 -0.3348014 ] \t0\ttrue\n",
            "(0)\t 766\t [ 1.6309123 -2.2338   ] \t0\ttrue\n",
            "(1)\t 767\t [-1.7665986  2.2589931] \t1\ttrue\n",
            "(0)\t 768\t [ 1.0153956 -1.4773186] \t0\ttrue\n",
            "(1)\t 769\t [-1.4780427  2.0836074] \t1\ttrue\n",
            "(1)\t 770\t [-1.5399987  1.8414512] \t1\ttrue\n",
            "(0)\t 771\t [-1.6301044  2.0257611] \t1\tfalse\n",
            "(0)\t 772\t [ 2.1072972 -2.6722357] \t0\ttrue\n",
            "(1)\t 773\t [-1.6175098  1.9217961] \t1\ttrue\n",
            "(0)\t 774\t [-1.6888018  2.1868844] \t1\tfalse\n",
            "(0)\t 775\t [ 2.174942 -2.619189] \t0\ttrue\n",
            "(1)\t 776\t [-1.5050521  2.0603178] \t1\ttrue\n",
            "(1)\t 777\t [-1.6106776  2.0667846] \t1\ttrue\n",
            "(1)\t 778\t [-1.4824458  2.043563 ] \t1\ttrue\n",
            "(0)\t 779\t [ 1.7947838 -2.1014593] \t0\ttrue\n",
            "(1)\t 780\t [-1.6547348  2.2285404] \t1\ttrue\n",
            "(0)\t 781\t [ 2.0412717 -2.5958633] \t0\ttrue\n",
            "(0)\t 782\t [ 1.922637  -2.4173474] \t0\ttrue\n",
            "(1)\t 783\t [-1.4570351  2.03784  ] \t1\ttrue\n",
            "(0)\t 784\t [-0.58387077  0.3738952 ] \t1\tfalse\n",
            "(1)\t 785\t [-1.602515   1.9340482] \t1\ttrue\n",
            "(0)\t 786\t [ 1.77357   -2.3382854] \t0\ttrue\n",
            "(1)\t 787\t [-1.5378156  1.9443258] \t1\ttrue\n",
            "(0)\t 788\t [ 1.9293448 -2.4219708] \t0\ttrue\n",
            "(0)\t 789\t [ 1.845736 -2.507675] \t0\ttrue\n",
            "(1)\t 790\t [-1.6806207  2.152538 ] \t1\ttrue\n",
            "(0)\t 791\t [ 1.7276961 -2.3266747] \t0\ttrue\n",
            "(1)\t 792\t [-1.4490167  2.0606823] \t1\ttrue\n",
            "(1)\t 793\t [ 1.4336212 -2.0255532] \t0\tfalse\n",
            "(0)\t 794\t [ 1.4336212 -2.0255532] \t0\ttrue\n",
            "(0)\t 795\t [ 2.0822217 -2.5695343] \t0\ttrue\n",
            "(1)\t 796\t [-0.7621141  0.605901 ] \t1\ttrue\n",
            "(1)\t 797\t [-1.5666753  1.9761708] \t1\ttrue\n",
            "(0)\t 798\t [ 1.2139493 -1.5293332] \t0\ttrue\n",
            "(0)\t 799\t [ 0.09608577 -0.2520392 ] \t0\ttrue\n",
            "(1)\t 800\t [ 0.09608577 -0.2520392 ] \t0\tfalse\n",
            "(0)\t 801\t [ 0.09608577 -0.2520392 ] \t0\ttrue\n",
            "(1)\t 802\t [ 0.09608577 -0.2520392 ] \t0\tfalse\n",
            "(1)\t 803\t [-1.2181622  1.33073  ] \t1\ttrue\n",
            "(0)\t 804\t [-0.8592956   0.74452937] \t1\tfalse\n",
            "(0)\t 805\t [ 1.6280757 -1.9908338] \t0\ttrue\n",
            "(1)\t 806\t [ 1.6816785 -2.0253806] \t0\tfalse\n",
            "(0)\t 807\t [ 2.0244982 -2.5326643] \t0\ttrue\n",
            "(1)\t 808\t [-1.7508785  2.1057518] \t1\ttrue\n",
            "(0)\t 809\t [ 2.0565248 -2.5817013] \t0\ttrue\n",
            "(1)\t 810\t [-1.4013275  1.936858 ] \t1\ttrue\n",
            "(0)\t 811\t [ 0.17238796 -0.54913265] \t0\ttrue\n",
            "(0)\t 812\t [ 1.845785  -2.4752114] \t0\ttrue\n",
            "(1)\t 813\t [-1.6940624  2.2728596] \t1\ttrue\n",
            "(0)\t 814\t [-0.31121373  0.20692082] \t1\tfalse\n",
            "(1)\t 815\t [-1.2944573  1.8019668] \t1\ttrue\n",
            "(0)\t 816\t [ 1.9453753 -2.4682033] \t0\ttrue\n",
            "(1)\t 817\t [ 1.2596742 -1.5784873] \t0\tfalse\n",
            "(0)\t 818\t [ 1.9204336 -2.5121374] \t0\ttrue\n",
            "(1)\t 819\t [-1.1567041  1.1719732] \t1\ttrue\n",
            "(0)\t 820\t [-0.3248519   0.11637385] \t1\tfalse\n",
            "(0)\t 821\t [ 1.9921175 -2.3589103] \t0\ttrue\n",
            "(0)\t 822\t [ 1.8397588 -2.3116648] \t0\ttrue\n",
            "(0)\t 823\t [-1.2588409  1.4389153] \t1\tfalse\n",
            "(0)\t 824\t [ 1.6553699 -2.1749878] \t0\ttrue\n",
            "(1)\t 825\t [-1.4368769  1.6376733] \t1\ttrue\n",
            "(0)\t 826\t [ 1.1095665 -1.4954036] \t0\ttrue\n",
            "(1)\t 827\t [-1.6401266  1.9659958] \t1\ttrue\n",
            "(0)\t 828\t [ 2.1193352 -2.460177 ] \t0\ttrue\n",
            "(1)\t 829\t [-1.5639204  2.1278653] \t1\ttrue\n",
            "(1)\t 830\t [-1.271047   1.9104022] \t1\ttrue\n",
            "(0)\t 831\t [ 1.7125155 -2.3471453] \t0\ttrue\n",
            "(1)\t 832\t [-1.5001594  2.075821 ] \t1\ttrue\n",
            "(0)\t 833\t [ 1.3320659 -1.846975 ] \t0\ttrue\n",
            "(0)\t 834\t [ 1.8438461 -2.2993414] \t0\ttrue\n",
            "(1)\t 835\t [-1.4650224  1.649866 ] \t1\ttrue\n",
            "(1)\t 836\t [-1.6511759  2.1516845] \t1\ttrue\n",
            "(0)\t 837\t [ 1.4681528 -1.9297265] \t0\ttrue\n",
            "(0)\t 838\t [ 1.4701248 -1.8154666] \t0\ttrue\n",
            "(0)\t 839\t [ 1.9871968 -2.5125809] \t0\ttrue\n",
            "(1)\t 840\t [-1.6624535  2.1562574] \t1\ttrue\n",
            "(1)\t 841\t [-1.624633  1.993786] \t1\ttrue\n",
            "(0)\t 842\t [ 1.78152   -2.3445992] \t0\ttrue\n",
            "(0)\t 843\t [ 1.6525005 -2.2554424] \t0\ttrue\n",
            "(1)\t 844\t [-1.5382996  1.7498982] \t1\ttrue\n",
            "(0)\t 845\t [ 0.25597614 -0.57566315] \t0\ttrue\n",
            "(1)\t 846\t [ 0.2934558 -0.6220786] \t0\tfalse\n",
            "(0)\t 847\t [ 1.6821153 -2.2039545] \t0\ttrue\n",
            "(1)\t 848\t [-1.1416538  1.0866611] \t1\ttrue\n",
            "(1)\t 849\t [ 0.43359256 -0.84843606] \t0\tfalse\n",
            "(0)\t 850\t [ 0.16055648 -0.4738316 ] \t0\ttrue\n",
            "(1)\t 851\t [-0.29829997  0.071069  ] \t1\ttrue\n",
            "(0)\t 852\t [ 1.5572169 -1.9540224] \t0\ttrue\n",
            "(0)\t 853\t [ 1.5896511 -2.174366 ] \t0\ttrue\n",
            "(1)\t 854\t [-1.6493827  2.152483 ] \t1\ttrue\n",
            "(0)\t 855\t [ 1.8995765 -2.303651 ] \t0\ttrue\n",
            "(0)\t 856\t [-0.2429132  0.0124442] \t1\tfalse\n",
            "(1)\t 857\t [ 0.0238791  -0.33280572] \t0\tfalse\n",
            "(0)\t 858\t [ 2.020994  -2.3716893] \t0\ttrue\n",
            "(1)\t 859\t [ 0.29707593 -0.60960966] \t0\tfalse\n",
            "(0)\t 860\t [ 0.29707593 -0.60960966] \t0\ttrue\n",
            "(0)\t 861\t [ 0.2279189 -0.58796  ] \t0\ttrue\n",
            "(1)\t 862\t [ 0.2279189 -0.58796  ] \t0\tfalse\n",
            "(0)\t 863\t [ 1.9462289 -2.2729273] \t0\ttrue\n",
            "(0)\t 864\t [ 1.8775395 -2.3921945] \t0\ttrue\n",
            "(1)\t 865\t [-1.5687122  2.1768239] \t1\ttrue\n",
            "(0)\t 866\t [-0.35767478  0.07116102] \t1\tfalse\n",
            "(1)\t 867\t [ 1.7142159 -2.282412 ] \t0\tfalse\n",
            "(0)\t 868\t [ 1.7803183 -2.2613156] \t0\ttrue\n",
            "(0)\t 869\t [ 1.7327403 -2.0672653] \t0\ttrue\n",
            "(0)\t 870\t [ 2.0685704 -2.4342005] \t0\ttrue\n",
            "(1)\t 871\t [-1.3078525  1.4173405] \t1\ttrue\n",
            "(0)\t 872\t [ 1.6030476 -2.0972002] \t0\ttrue\n",
            "(0)\t 873\t [ 1.9809302 -2.3277977] \t0\ttrue\n",
            "(1)\t 874\t [-1.7204365  2.2841485] \t1\ttrue\n",
            "(1)\t 875\t [-1.49525    2.1563632] \t1\ttrue\n",
            "(0)\t 876\t [-1.6456178  1.9077083] \t1\tfalse\n",
            "(0)\t 877\t [-0.2698583  0.0171967] \t1\tfalse\n",
            "(1)\t 878\t [-1.6426219  2.0894198] \t1\ttrue\n",
            "(1)\t 879\t [ 1.6000761 -2.1705942] \t0\tfalse\n",
            "(0)\t 880\t [ 1.6867566 -1.9599462] \t0\ttrue\n",
            "(0)\t 881\t [ 1.6772635 -2.1706543] \t0\ttrue\n",
            "(1)\t 882\t [-0.63925976  0.46429512] \t1\ttrue\n",
            "(1)\t 883\t [ 0.12082653 -0.45068455] \t0\tfalse\n",
            "(0)\t 884\t [ 0.12082653 -0.45068455] \t0\ttrue\n",
            "(0)\t 885\t [ 1.5725493 -2.0030048] \t0\ttrue\n",
            "(1)\t 886\t [-1.5026332  1.7348938] \t1\ttrue\n",
            "(0)\t 887\t [ 1.3097826 -1.5598879] \t0\ttrue\n",
            "(1)\t 888\t [-1.5627961  1.7595836] \t1\ttrue\n",
            "(0)\t 889\t [ 2.0183966 -2.5787199] \t0\ttrue\n",
            "(1)\t 890\t [-1.2319103  1.207971 ] \t1\ttrue\n",
            "(0)\t 891\t [ 0.23902789 -0.56795   ] \t0\ttrue\n",
            "(1)\t 892\t [ 0.23902789 -0.56795   ] \t0\tfalse\n",
            "(1)\t 893\t [ 1.3964547 -1.68558  ] \t0\tfalse\n",
            "(0)\t 894\t [ 2.0571177 -2.5920687] \t0\ttrue\n",
            "(1)\t 895\t [-1.4537657  1.7273151] \t1\ttrue\n",
            "(0)\t 896\t [ 2.1264894 -2.5026505] \t0\ttrue\n",
            "(0)\t 897\t [ 1.9773079 -2.3751423] \t0\ttrue\n",
            "(1)\t 898\t [ 1.4807179 -1.8602061] \t0\tfalse\n",
            "(0)\t 899\t [ 1.703467  -2.2481587] \t0\ttrue\n",
            "(1)\t 900\t [-1.5981231  2.153716 ] \t1\ttrue\n",
            "Number of true predictions: 760\n",
            "Number of false predictions: 140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIgsWWCTKdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18e60e8c-45f1-477d-cb50-4eaa5a099e20"
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 84.35072142064372 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print('True positives: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "391fcb7e-3cc1-4353-ce93-bc976e740303"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI2K1bqaR5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "359078e7-7ce2-4dd1-ba95-0af39cc26021"
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.691\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QowNXrYZSRMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "b3d79abe-dd3d-4a68-8062-cdc7b58f713c"
      },
      "source": [
        "print(flat_predictions)\n",
        "print(\"************\")\n",
        "print(flat_true_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 0 0 0\n",
            " 1 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 0 1 1 1 0 1 1 0 0\n",
            " 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 0 1 0 1\n",
            " 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 0 1\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1\n",
            " 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0\n",
            " 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 1 0 1 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
            " 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1\n",
            " 1 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 0 1 0 1 1 1 0\n",
            " 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 1 1\n",
            " 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1\n",
            " 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1\n",
            " 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1\n",
            " 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0\n",
            " 0 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1\n",
            " 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 0\n",
            " 1 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0\n",
            " 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 0 0 1\n",
            " 0 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 0 0 1\n",
            " 1 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1\n",
            " 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 1\n",
            " 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1\n",
            " 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 1\n",
            " 0 1 0 0 0 0 1 0 0 0 0 1]\n",
            "************\n",
            "[1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPTA_ePt0wVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "bd0630a2-0ab3-46cf-a784-03931faf30e8"
      },
      "source": [
        "Model_location = input(\"Enter the location to save your model : \")    #example - task1hindi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the location to save your model : task1hindi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7dd85870-2a89-4d6b-f1b5-cc2ab387dc29"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-'+Model_location    #by default the model is stored here , you can change this location according to your convinience\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task1hindi\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task1hindi/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1hindi/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1hindi/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    }
  ]
}