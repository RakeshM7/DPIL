{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RakeshM7/DPIL-BERT/blob/master/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN2gN6gr8nOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "outputId": "b0c6d72a-c965-4c7d-e471-dbc3ba006c84"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "!pip install transformers"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agL2oUFJO9BM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1d355c87-a61a-47a1-ec1d-b3363775021c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4udGSQsHzAc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "70b9fed4-1f25-4f60-c9be-47efbb755a37"
      },
      "source": [
        "taskid= input(\"Enter the taskname : \")    #example - task1\n",
        "language= input(\"Enter the language: \")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the taskname : task1\n",
            "Enter the language: tamil\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roq3nEGz9wvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "88d22502-760b-47cf-f41d-2c5306f19def"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/\"+language+\"/\"+taskid+language+\".tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data.\n",
        "df.sample(10)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 2,500\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2499</th>\n",
              "      <td>TAM2500</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>'தமிழக முதல்வராக, ஓ.பன்னீர்செல்வம் பொறுப்பேற்ற...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>914</th>\n",
              "      <td>TAM0915</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>நிதிஷ்குமாரை தாக்கி பேசிய லாலு கட்சி எம்.பி.க்...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2359</th>\n",
              "      <td>TAM2360</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>தமிழக முன்னாள் முதல்வர் ஜெயலலிதா மீதான சொத்து ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1434</th>\n",
              "      <td>TAM1435</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>பிரதமருடனான சந்திப்பு முடிந்ததும் டெல்லியில் இ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1918</th>\n",
              "      <td>TAM1919</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>போலீஸ் நிலையத்திலிருந்து வெளியே வந்த போலீசார் ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1734</th>\n",
              "      <td>TAM1735</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>கடல்சார் அச்சுறுத்தல்களால் நாட்டின் பொருளாதாரம...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1825</th>\n",
              "      <td>TAM1826</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>உள்ளாட்சித் தேர்தல் இடஒதுக்கீடு தொடர்பாக திரும...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1446</th>\n",
              "      <td>TAM1447</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>கலாபவன்மணி அருந்திய மது மூலம் இந்த நச்சு பொருள...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1272</th>\n",
              "      <td>TAM1273</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>காட்டு யானையை பிடிக்க வனத் துறையினர் திட்டமிட்...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>TAM0325</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>கிள்ளியூர் சட்டமன்றத் தொகுதி த.மா.கா. வேட்பாளர...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "2499         TAM2500  ...  'தமிழக முதல்வராக, ஓ.பன்னீர்செல்வம் பொறுப்பேற்ற...\n",
              "914          TAM0915  ...  நிதிஷ்குமாரை தாக்கி பேசிய லாலு கட்சி எம்.பி.க்...\n",
              "2359         TAM2360  ...  தமிழக முன்னாள் முதல்வர் ஜெயலலிதா மீதான சொத்து ...\n",
              "1434         TAM1435  ...  பிரதமருடனான சந்திப்பு முடிந்ததும் டெல்லியில் இ...\n",
              "1918         TAM1919  ...  போலீஸ் நிலையத்திலிருந்து வெளியே வந்த போலீசார் ...\n",
              "1734         TAM1735  ...  கடல்சார் அச்சுறுத்தல்களால் நாட்டின் பொருளாதாரம...\n",
              "1825         TAM1826  ...  உள்ளாட்சித் தேர்தல் இடஒதுக்கீடு தொடர்பாக திரும...\n",
              "1446         TAM1447  ...  கலாபவன்மணி அருந்திய மது மூலம் இந்த நச்சு பொருள...\n",
              "1272         TAM1273  ...  காட்டு யானையை பிடிக்க வனத் துறையினர் திட்டமிட்...\n",
              "324          TAM0325  ...  கிள்ளியூர் சட்டமன்றத் தொகுதி த.மா.கா. வேட்பாளர...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK0Dm2839_fM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0acba11c-f92d-4f47-cfce-bc0e718a42a4"
      },
      "source": [
        "df.loc[df.label == 0].sample(5)[['sentence', 'label']]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1123</th>\n",
              "      <td>டெல்லி சட்டபேரவை விதிகளின்படி எம்.எல்.ஏ.க்களுக...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1318</th>\n",
              "      <td>உத்தரபிரதேச மாநிலம் அலகாபாத்தில் பாரதீய ஜனதா த...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1360</th>\n",
              "      <td>பவித்ராவை போலீசார் எச்சரித்து அனுப்பி வைத்தனர்...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1450</th>\n",
              "      <td>அறிக்கையில் கலாபவன்மணியின் உடலில் மெத்தனால் என...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2312</th>\n",
              "      <td>காஷ்மீரில் எல்லைக்கட்டுப்பாட்டு பகுதி வழியாக த...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               sentence  label\n",
              "1123  டெல்லி சட்டபேரவை விதிகளின்படி எம்.எல்.ஏ.க்களுக...      0\n",
              "1318  உத்தரபிரதேச மாநிலம் அலகாபாத்தில் பாரதீய ஜனதா த...      0\n",
              "1360  பவித்ராவை போலீசார் எச்சரித்து அனுப்பி வைத்தனர்...      0\n",
              "1450  அறிக்கையில் கலாபவன்மணியின் உடலில் மெத்தனால் என...      0\n",
              "2312  காஷ்மீரில் எல்லைக்கட்டுப்பாட்டு பகுதி வழியாக த...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64AfSL-V-Ij5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60lFABu1-KAE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a473b0d-77b8-44c6-98f4-8fdec790d37f"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=True)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqawKMwS-o5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4e0fde5a-416c-48f8-9419-34c8f321a9f9"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Tokenized:  ['ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'பி', '##ர', '##ச', '##ார', '##ம', 'ச', '##ெ', '##ய', '##தா', '##ர', '.', '<', 'eo', '##l', '>', 'தி', '.', 'மு', '.', 'க', '.', ',', 'வ', '##ே', '##ட', '##பா', '##ள', '##ர', 'ஸ', '##ட', '##ால', '##ி', '##ன', 'ப', '##ே', '##ா', '##ட', '##டி', '##ய', '##ி', '##டு', '##ம', 'ச', '##ங', '##க', '##ரா', '##பு', '##ர', '##ம', 'த', '##ெ', '##ாக', '##ு', '##திய', '##ில', 'சி', '##ன', '##ன', 'ச', '##ே', '##ல', '##ம', 'பகுதி', '##ய', '##ில', 'ந', '##டை', '##ப', '##ய', '##ண', '##மாக', 'ச', '##ெ', '##ன', '##று', 'ஓ', '##ட', '##டு', 'ச', '##ே', '##க', '##ரி', '##த', '##தா', '##ர', '.']\n",
            "Token IDs:  [1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKGzPxF1AUUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "7cd8d77d-4096-4804-eff7-bee0031192f8"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:  சங்கராபுரம் தொகுதியில் போட்டியிடும் ஸ்டாலின் நடைபயணமாக சென்று பிரசாரம் செய்தார்.<eol>தி.மு.க., வேட்பாளர் ஸ்டாலின் போட்டியிடும் சங்கராபுரம் தொகுதியில் சின்ன சேலம் பகுதியில் நடைபயணமாக சென்று ஓட்டு சேகரித்தார்.\n",
            "Token IDs: [101, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1172, 35186, 71071, 20242, 17506, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 37076, 21060, 59245, 81773, 39123, 1154, 111312, 15220, 99099, 21060, 119, 133, 13934, 10161, 135, 55993, 119, 95512, 119, 1152, 119, 117, 1170, 18827, 35186, 88626, 55186, 21060, 1172, 35186, 71071, 20242, 17506, 1162, 18827, 15472, 35186, 24171, 15220, 20242, 35667, 39123, 1154, 111305, 19894, 74405, 29972, 21060, 39123, 1159, 111312, 24515, 18660, 85056, 94978, 86625, 17506, 17506, 1154, 18827, 27885, 39123, 81512, 15220, 94978, 1160, 51177, 46168, 15220, 40397, 22093, 1154, 111312, 17506, 21800, 1150, 35186, 35667, 1154, 18827, 19894, 21426, 31484, 99099, 21060, 119, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv39u2kLAo9b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f00e0f53-fc7b-4de1-80d5-b0f3396dfe08"
      },
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max sentence length:  306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GkPkFAsKR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "85b90226-672c-4589-93bb-7524e61d7be3"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 64\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 64 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUKKZrYgAuTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfmeSm3zAxOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a7ebdaac-5a59-4059-ae60-a45487cb578a"
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "print(train_inputs)\n",
        "print(train_labels)\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.2)\n",
        "#print(train_masks)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   101   1154  35186 ...  55186  95512  31484]\n",
            " [   101   1170 111305 ...   1154 111312  15220]\n",
            " [   101   1163  14124 ...  28065    119    133]\n",
            " ...\n",
            " [   101   1163  27883 ...  27885   1165  18827]\n",
            " [   101  49189  66171 ...  86728  81773  20242]\n",
            " [   101   1142  46168 ...  37076  24171  19894]]\n",
            "[1 0 1 ... 1 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6BSzcZ-A8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P2ab-k8GgLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvYAxocaGzaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5dd7f52b-cc2d-47d1-ec71-72ee87ff76a7"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xB1woJrHCZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "066dce29-555a-46e0-e32f-9b9640ecdf0a"
      },
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (119547, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NhjDCrtHGh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBK2E_PaHKQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOO83LgEHQYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2jmuLjzHUP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6dh8MSXHXCv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "e5ea0b9b-2c9d-4744-8b26-32f3c0463386"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of     63.    Elapsed: 0:00:09.\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:00:14\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.87\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Er8Tzb9xj-D",
        "colab_type": "text"
      },
      "source": [
        "Uncomment if you want graphical representaion of training vs epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifs2QgT9HeUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# % matplotlib inline\n",
        "\n",
        "# import seaborn as sns\n",
        "\n",
        "# # Use plot styling from seaborn.\n",
        "# sns.set(style='darkgrid')\n",
        "\n",
        "# # Increase the plot size and font size.\n",
        "# sns.set(font_scale=1.5)\n",
        "# plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# # Plot the learning curve.\n",
        "# plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# # Label the plot.\n",
        "# plt.title(\"Training loss\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "\n",
        "# plt.show()"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJBFn_4nziAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TestData_name = input(\"Enter the name of the test data tsv file: \")     #example - task1hindi-test"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3sCgA3MK9B2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c53d212a-d539-4129-cd1e-78cd8b8a70ec"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/FYP/bert/glue/cola/testdata/\"+taskid+language+\"-test.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 900\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsFeNFSLIkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8fda5594-3623-4e8b-8d28-99a586d2454b"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "  # # print(predictions)\n",
        "  #print(true_labels)\n",
        "print('    DONE.')\n",
        "\n",
        "#print(true_labels)\n",
        "print(\"*************************\")\n",
        "\n",
        "#print(len(predictions))\n",
        "#print(outputs)\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 900 test sentences...\n",
            "    DONE.\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaKraEhj0OxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Results_path = input(\"Enter the filename of the result file : \")  #example - task1hindi-results"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgpMTRW-jMtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.savetxt(\"/content/drive/My Drive/FYP/bert/glue/cola/\"+taskid+language+\"-results.txt\",predictions, fmt=\"%s\")\n",
        "\n"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGGtb-jicKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b6bc131-108e-4aa9-b2c0-776059c86876"
      },
      "source": [
        "#this block of code is meant only for debugging purposes\n",
        "count_NP=0\n",
        "count_P=0\n",
        "count_line=1\n",
        "x=-1\n",
        "true_count,false_count=0,0\n",
        "print(\"label \\t sno\\tlogit\\t\\t\\t\\t\\tindex\")\n",
        "for i in range(len(predictions)):\n",
        "  for j in range(len(predictions[i])):\n",
        "    print(\"(\",end=\"\")\n",
        "    print(true_labels[i][j],end=\"\")\n",
        "    print(\")\",end=\"\")\n",
        "    print(\"\\t\",count_line,end=\"\")\n",
        "    # print(\"-  \",end=\"\")\n",
        "    if(predictions[i][j][0]>predictions[i][j][1] ):\n",
        "      #count_P+=1 \n",
        "      #print(\" Non Paraphrase         \",end=\"\")\n",
        "      print(\"\\t\",predictions[i][j],\"\\t0\\t\",end=\"\")\n",
        "      x=0\n",
        "    elif(predictions[i][j][1]>predictions[i][j][0] ):\n",
        "      #print(\" Paraphrase     \",end=\"\")\n",
        "     # count_NP+=1      \n",
        "      print(\"\\t\",predictions[i][j],\"\\t1\\t\",end=\"\")\n",
        "      x=1\n",
        "    # elif(predictions[i][j][2]>predictions[i][j][0] and predictions[i][j][2]>predictions[i][j][1]):\n",
        "    #   #print(\" Semi Paraphrase     \",end=\"\")\n",
        "    #   #count_NP+=1      \n",
        "    #   print(\"\\t\",predictions[i][j][2],\"\\t2\\t\",end=\"\")\n",
        "    #   x=1\n",
        "    count_line+=1\n",
        "    #print(\"\\t\",predictions[i][j],\"\\t2\\t\",end=\"\")\n",
        "\n",
        "    if(true_labels[i][j]==x):\n",
        "      true_count+=1\n",
        "      print(\"true\")\n",
        "    else:\n",
        "      print(\"false\")\n",
        "      false_count+=1\n",
        "\n",
        "print(\"Number of true predictions:\",true_count)\n",
        "print(\"Number of false predictions:\",false_count)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label \t sno\tlogit\t\t\t\t\tindex\n",
            "(1)\t 1\t [-1.4056864  1.1224213] \t1\ttrue\n",
            "(1)\t 2\t [ 0.4220209 -0.2713189] \t0\tfalse\n",
            "(1)\t 3\t [-0.15201858 -0.05631719] \t1\ttrue\n",
            "(1)\t 4\t [ 2.1474187 -2.0724957] \t0\tfalse\n",
            "(0)\t 5\t [ 2.3748338 -2.1995497] \t0\ttrue\n",
            "(1)\t 6\t [ 1.3801392 -1.330132 ] \t0\tfalse\n",
            "(0)\t 7\t [ 1.0320913 -1.2102704] \t0\ttrue\n",
            "(0)\t 8\t [ 1.2649304 -1.0086398] \t0\ttrue\n",
            "(0)\t 9\t [ 2.2569845 -2.21088  ] \t0\ttrue\n",
            "(0)\t 10\t [ 0.94438577 -0.9866816 ] \t0\ttrue\n",
            "(1)\t 11\t [ 1.172665  -1.0374074] \t0\tfalse\n",
            "(0)\t 12\t [ 2.107829  -2.1492207] \t0\ttrue\n",
            "(0)\t 13\t [ 1.8568444 -1.8085755] \t0\ttrue\n",
            "(0)\t 14\t [ 2.2149634 -2.2518818] \t0\ttrue\n",
            "(0)\t 15\t [ 1.4074455 -1.3135872] \t0\ttrue\n",
            "(0)\t 16\t [ 0.5143831 -0.7276721] \t0\ttrue\n",
            "(0)\t 17\t [ 2.097631 -2.134749] \t0\ttrue\n",
            "(0)\t 18\t [-1.1761063  0.9967002] \t1\tfalse\n",
            "(0)\t 19\t [ 1.0605215 -0.8780223] \t0\ttrue\n",
            "(0)\t 20\t [ 2.102511  -2.1246185] \t0\ttrue\n",
            "(0)\t 21\t [ 2.0586452 -1.9444143] \t0\ttrue\n",
            "(1)\t 22\t [ 0.2531671  -0.46543908] \t0\tfalse\n",
            "(0)\t 23\t [ 1.9324844 -2.0507338] \t0\ttrue\n",
            "(0)\t 24\t [ 1.3134975 -1.532841 ] \t0\ttrue\n",
            "(0)\t 25\t [ 1.6645912 -1.7854478] \t0\ttrue\n",
            "(0)\t 26\t [ 1.00776   -1.1443224] \t0\ttrue\n",
            "(0)\t 27\t [ 1.6978517 -1.7328312] \t0\ttrue\n",
            "(0)\t 28\t [-1.2956132  1.044883 ] \t1\tfalse\n",
            "(0)\t 29\t [ 2.0054066 -2.0956156] \t0\ttrue\n",
            "(0)\t 30\t [ 1.8944896 -1.8273346] \t0\ttrue\n",
            "(0)\t 31\t [-1.837333   1.4866812] \t1\tfalse\n",
            "(0)\t 32\t [ 1.5540054 -1.602    ] \t0\ttrue\n",
            "(0)\t 33\t [ 1.4618492 -1.6926033] \t0\ttrue\n",
            "(1)\t 34\t [ 2.1344407 -1.9029423] \t0\tfalse\n",
            "(1)\t 35\t [ 2.1344407 -1.9029423] \t0\tfalse\n",
            "(1)\t 36\t [ 2.1992831 -2.1710975] \t0\tfalse\n",
            "(0)\t 37\t [-0.8073464  0.6694924] \t1\tfalse\n",
            "(1)\t 38\t [ 2.1238115 -2.0849662] \t0\tfalse\n",
            "(0)\t 39\t [ 1.7532201 -1.9415433] \t0\ttrue\n",
            "(0)\t 40\t [0.01587896 0.21809122] \t1\tfalse\n",
            "(1)\t 41\t [ 1.3534722 -1.3241851] \t0\tfalse\n",
            "(0)\t 42\t [ 0.42848814 -0.72362316] \t0\ttrue\n",
            "(1)\t 43\t [-1.187729  0.842344] \t1\ttrue\n",
            "(1)\t 44\t [ 1.0924926 -0.9772764] \t0\tfalse\n",
            "(1)\t 45\t [ 1.0924926 -0.9772764] \t0\tfalse\n",
            "(0)\t 46\t [ 1.3604622 -1.5807753] \t0\ttrue\n",
            "(0)\t 47\t [ 1.9493579 -1.9702747] \t0\ttrue\n",
            "(1)\t 48\t [ 1.9066393 -2.0332432] \t0\tfalse\n",
            "(0)\t 49\t [-0.33760333  0.2907533 ] \t1\tfalse\n",
            "(0)\t 50\t [ 1.8108947 -1.7125739] \t0\ttrue\n",
            "(0)\t 51\t [ 0.29857036 -0.051945  ] \t0\ttrue\n",
            "(0)\t 52\t [ 0.7591213  -0.50420177] \t0\ttrue\n",
            "(0)\t 53\t [ 2.3444104 -2.23654  ] \t0\ttrue\n",
            "(0)\t 54\t [ 2.1494567 -2.0833125] \t0\ttrue\n",
            "(0)\t 55\t [ 2.301202  -2.2252667] \t0\ttrue\n",
            "(1)\t 56\t [ 1.1434883 -1.037884 ] \t0\tfalse\n",
            "(1)\t 57\t [-1.8717216  1.5936041] \t1\ttrue\n",
            "(0)\t 58\t [ 1.1282661 -1.2381525] \t0\ttrue\n",
            "(0)\t 59\t [ 2.080005  -2.0907636] \t0\ttrue\n",
            "(1)\t 60\t [ 0.5182638  -0.58283085] \t0\tfalse\n",
            "(0)\t 61\t [ 2.0018947 -1.921221 ] \t0\ttrue\n",
            "(1)\t 62\t [ 1.1345595 -1.0755529] \t0\tfalse\n",
            "(1)\t 63\t [-1.7714901  1.4219046] \t1\ttrue\n",
            "(1)\t 64\t [ 0.4484465 -0.2556286] \t0\tfalse\n",
            "(1)\t 65\t [ 0.4484465 -0.2556286] \t0\tfalse\n",
            "(0)\t 66\t [ 0.71748877 -0.5371019 ] \t0\ttrue\n",
            "(0)\t 67\t [ 1.8082165 -1.605691 ] \t0\ttrue\n",
            "(0)\t 68\t [ 1.0743642 -1.0360624] \t0\ttrue\n",
            "(0)\t 69\t [ 1.8659887 -1.841226 ] \t0\ttrue\n",
            "(0)\t 70\t [ 2.1123936 -1.9366195] \t0\ttrue\n",
            "(0)\t 71\t [ 0.7222613  -0.78301066] \t0\ttrue\n",
            "(1)\t 72\t [-1.7773056  1.4996868] \t1\ttrue\n",
            "(1)\t 73\t [-1.7773056  1.4996868] \t1\ttrue\n",
            "(1)\t 74\t [-0.9061374  0.9148402] \t1\ttrue\n",
            "(1)\t 75\t [-0.9061374  0.9148402] \t1\ttrue\n",
            "(1)\t 76\t [ 1.9198816 -1.7962651] \t0\tfalse\n",
            "(1)\t 77\t [ 1.9198816 -1.7962651] \t0\tfalse\n",
            "(1)\t 78\t [-1.3993018  1.2663386] \t1\ttrue\n",
            "(0)\t 79\t [ 2.0969083 -1.9523602] \t0\ttrue\n",
            "(0)\t 80\t [ 1.9166732 -2.089774 ] \t0\ttrue\n",
            "(0)\t 81\t [ 2.3513496 -2.2231567] \t0\ttrue\n",
            "(1)\t 82\t [-1.9375362  1.6027375] \t1\ttrue\n",
            "(0)\t 83\t [ 2.0793712 -1.9977636] \t0\ttrue\n",
            "(0)\t 84\t [ 1.9850451 -1.947682 ] \t0\ttrue\n",
            "(1)\t 85\t [ 2.198702  -2.0858457] \t0\tfalse\n",
            "(1)\t 86\t [ 2.1996086 -2.0036657] \t0\tfalse\n",
            "(1)\t 87\t [ 2.1693432 -2.1984267] \t0\tfalse\n",
            "(0)\t 88\t [ 1.7702056 -1.7167201] \t0\ttrue\n",
            "(0)\t 89\t [ 1.1677686 -0.9706526] \t0\ttrue\n",
            "(1)\t 90\t [-1.7404549  1.42045  ] \t1\ttrue\n",
            "(1)\t 91\t [-1.667027  1.464795] \t1\ttrue\n",
            "(1)\t 92\t [-1.2718649  0.9753593] \t1\ttrue\n",
            "(0)\t 93\t [ 0.03447812 -0.10535471] \t0\ttrue\n",
            "(0)\t 94\t [ 0.6011928 -0.7113474] \t0\ttrue\n",
            "(0)\t 95\t [ 2.1589224 -2.0629208] \t0\ttrue\n",
            "(0)\t 96\t [ 1.622882  -1.6388237] \t0\ttrue\n",
            "(0)\t 97\t [ 2.0602279 -1.9451813] \t0\ttrue\n",
            "(0)\t 98\t [ 2.2904508 -2.251207 ] \t0\ttrue\n",
            "(0)\t 99\t [ 1.8732874 -1.8687534] \t0\ttrue\n",
            "(0)\t 100\t [ 1.8818517 -1.8946525] \t0\ttrue\n",
            "(1)\t 101\t [-1.963928   1.6133211] \t1\ttrue\n",
            "(1)\t 102\t [ 2.204006 -2.154328] \t0\tfalse\n",
            "(1)\t 103\t [ 1.6808407 -1.7177731] \t0\tfalse\n",
            "(1)\t 104\t [ 1.2263341 -1.0962478] \t0\tfalse\n",
            "(0)\t 105\t [ 1.8839989 -1.8552239] \t0\ttrue\n",
            "(1)\t 106\t [-1.3140547  1.140045 ] \t1\ttrue\n",
            "(0)\t 107\t [ 2.0160403 -1.9185393] \t0\ttrue\n",
            "(0)\t 108\t [ 1.5320688 -1.6105435] \t0\ttrue\n",
            "(0)\t 109\t [ 2.0497448 -2.1080768] \t0\ttrue\n",
            "(0)\t 110\t [ 1.4762866 -1.3372545] \t0\ttrue\n",
            "(0)\t 111\t [ 1.7065641 -1.4907968] \t0\ttrue\n",
            "(1)\t 112\t [-0.02112583  0.21764776] \t1\ttrue\n",
            "(1)\t 113\t [ 1.4140191 -1.2121888] \t0\tfalse\n",
            "(1)\t 114\t [ 2.2246487 -2.121887 ] \t0\tfalse\n",
            "(1)\t 115\t [ 1.5886052 -1.4396074] \t0\tfalse\n",
            "(0)\t 116\t [ 2.1037302 -2.163164 ] \t0\ttrue\n",
            "(0)\t 117\t [-0.16405058  0.39451134] \t1\tfalse\n",
            "(0)\t 118\t [ 1.0826793 -1.2197375] \t0\ttrue\n",
            "(0)\t 119\t [ 2.3473184 -2.2433968] \t0\ttrue\n",
            "(0)\t 120\t [ 2.180754  -2.0527666] \t0\ttrue\n",
            "(0)\t 121\t [ 1.1183143 -1.2375464] \t0\ttrue\n",
            "(0)\t 122\t [ 1.8922666 -2.012695 ] \t0\ttrue\n",
            "(1)\t 123\t [-0.65734667  0.39670914] \t1\ttrue\n",
            "(0)\t 124\t [ 2.2306411 -2.1512427] \t0\ttrue\n",
            "(0)\t 125\t [ 2.2372699 -2.2107139] \t0\ttrue\n",
            "(0)\t 126\t [ 1.6715429 -1.6597052] \t0\ttrue\n",
            "(1)\t 127\t [ 2.2534974 -2.143028 ] \t0\tfalse\n",
            "(1)\t 128\t [ 2.2534974 -2.143028 ] \t0\tfalse\n",
            "(0)\t 129\t [ 1.5289545 -1.7804656] \t0\ttrue\n",
            "(0)\t 130\t [ 2.220845  -2.1526334] \t0\ttrue\n",
            "(0)\t 131\t [ 2.026875  -2.0536342] \t0\ttrue\n",
            "(0)\t 132\t [ 2.1640663 -2.0395896] \t0\ttrue\n",
            "(1)\t 133\t [ 2.2623463 -2.1967463] \t0\tfalse\n",
            "(0)\t 134\t [ 1.9394214 -1.9229388] \t0\ttrue\n",
            "(0)\t 135\t [ 2.27278   -2.2054932] \t0\ttrue\n",
            "(0)\t 136\t [ 1.5558845 -1.796109 ] \t0\ttrue\n",
            "(0)\t 137\t [ 0.3298139  -0.45948195] \t0\ttrue\n",
            "(0)\t 138\t [ 1.8907979 -2.053425 ] \t0\ttrue\n",
            "(0)\t 139\t [ 2.3848383 -2.220817 ] \t0\ttrue\n",
            "(0)\t 140\t [ 1.9102615 -1.920536 ] \t0\ttrue\n",
            "(0)\t 141\t [ 2.1293523 -2.153793 ] \t0\ttrue\n",
            "(0)\t 142\t [ 0.78105474 -0.5318985 ] \t0\ttrue\n",
            "(1)\t 143\t [ 2.2695346 -2.187411 ] \t0\tfalse\n",
            "(1)\t 144\t [ 1.240087  -1.3043796] \t0\tfalse\n",
            "(1)\t 145\t [ 2.0425692 -1.98962  ] \t0\tfalse\n",
            "(0)\t 146\t [ 2.318154  -2.2147043] \t0\ttrue\n",
            "(0)\t 147\t [-1.5626755  1.2717997] \t1\tfalse\n",
            "(1)\t 148\t [ 2.0365474 -2.0258687] \t0\tfalse\n",
            "(1)\t 149\t [ 2.0365474 -2.0258687] \t0\tfalse\n",
            "(1)\t 150\t [ 1.9236405 -1.9152328] \t0\tfalse\n",
            "(1)\t 151\t [ 1.9236405 -1.9152328] \t0\tfalse\n",
            "(1)\t 152\t [ 1.8073857 -1.7495883] \t0\tfalse\n",
            "(1)\t 153\t [ 1.8073857 -1.7495883] \t0\tfalse\n",
            "(0)\t 154\t [ 2.1816442 -2.1556246] \t0\ttrue\n",
            "(1)\t 155\t [ 0.35522667 -0.09205654] \t0\tfalse\n",
            "(1)\t 156\t [ 0.35522667 -0.09205654] \t0\tfalse\n",
            "(1)\t 157\t [ 2.1904836 -2.2357776] \t0\tfalse\n",
            "(0)\t 158\t [ 1.8009305 -1.9962022] \t0\ttrue\n",
            "(0)\t 159\t [ 2.3121088 -2.2136953] \t0\ttrue\n",
            "(1)\t 160\t [ 1.5297807 -1.4080689] \t0\tfalse\n",
            "(0)\t 161\t [ 1.2978861 -1.328583 ] \t0\ttrue\n",
            "(1)\t 162\t [ 1.7679063 -1.8136146] \t0\tfalse\n",
            "(0)\t 163\t [ 2.2398076 -2.145461 ] \t0\ttrue\n",
            "(0)\t 164\t [ 0.14994723 -0.2960113 ] \t0\ttrue\n",
            "(0)\t 165\t [ 2.2401576 -2.2233365] \t0\ttrue\n",
            "(0)\t 166\t [-1.376992   1.1718649] \t1\tfalse\n",
            "(0)\t 167\t [ 2.264138  -2.1749666] \t0\ttrue\n",
            "(1)\t 168\t [ 0.97435534 -0.8149873 ] \t0\tfalse\n",
            "(1)\t 169\t [ 1.7781649 -1.9064795] \t0\tfalse\n",
            "(1)\t 170\t [ 1.7781649 -1.9064795] \t0\tfalse\n",
            "(0)\t 171\t [-0.8981865  0.9583136] \t1\tfalse\n",
            "(1)\t 172\t [ 0.9571    -0.8651158] \t0\tfalse\n",
            "(1)\t 173\t [ 1.0923944 -1.0091279] \t0\tfalse\n",
            "(1)\t 174\t [ 2.0818691 -2.0666077] \t0\tfalse\n",
            "(0)\t 175\t [-2.0378435  1.7401034] \t1\tfalse\n",
            "(1)\t 176\t [ 2.1567934 -2.1022253] \t0\tfalse\n",
            "(0)\t 177\t [ 1.836551  -1.8923402] \t0\ttrue\n",
            "(0)\t 178\t [ 2.2082994 -2.0998504] \t0\ttrue\n",
            "(0)\t 179\t [ 2.2500656 -2.100517 ] \t0\ttrue\n",
            "(0)\t 180\t [ 2.3178227 -2.2632778] \t0\ttrue\n",
            "(0)\t 181\t [ 2.014619  -1.9693238] \t0\ttrue\n",
            "(0)\t 182\t [ 2.1620123 -2.0557644] \t0\ttrue\n",
            "(0)\t 183\t [ 1.4534986 -1.3284184] \t0\ttrue\n",
            "(0)\t 184\t [ 1.2810624 -1.2469234] \t0\ttrue\n",
            "(0)\t 185\t [ 1.9453813 -2.1402144] \t0\ttrue\n",
            "(0)\t 186\t [ 2.2778313 -2.1869519] \t0\ttrue\n",
            "(1)\t 187\t [ 0.6736475 -0.8103361] \t0\tfalse\n",
            "(1)\t 188\t [-1.1982365  1.0094852] \t1\ttrue\n",
            "(0)\t 189\t [ 0.4039818  -0.14444631] \t0\ttrue\n",
            "(1)\t 190\t [ 1.0608007 -0.8217771] \t0\tfalse\n",
            "(0)\t 191\t [ 2.0007732 -2.0029309] \t0\ttrue\n",
            "(0)\t 192\t [ 1.9442941 -1.8295649] \t0\ttrue\n",
            "(1)\t 193\t [ 0.0347735  -0.06885819] \t0\tfalse\n",
            "(1)\t 194\t [ 1.6902682 -1.9067829] \t0\tfalse\n",
            "(0)\t 195\t [ 0.6849211 -0.8305658] \t0\ttrue\n",
            "(1)\t 196\t [ 0.36570895 -0.43668193] \t0\tfalse\n",
            "(1)\t 197\t [ 1.7307496 -1.6020447] \t0\tfalse\n",
            "(1)\t 198\t [ 0.29163477 -0.07544963] \t0\tfalse\n",
            "(1)\t 199\t [ 0.31720626 -0.07827669] \t0\tfalse\n",
            "(1)\t 200\t [ 0.31720626 -0.07827669] \t0\tfalse\n",
            "(0)\t 201\t [ 1.7591785 -1.8152854] \t0\ttrue\n",
            "(0)\t 202\t [ 0.95006245 -1.1177737 ] \t0\ttrue\n",
            "(1)\t 203\t [ 2.0105824 -1.8354597] \t0\tfalse\n",
            "(1)\t 204\t [ 2.0105824 -1.8354597] \t0\tfalse\n",
            "(1)\t 205\t [ 1.1179744 -0.994321 ] \t0\tfalse\n",
            "(1)\t 206\t [ 2.204704  -2.0403206] \t0\tfalse\n",
            "(0)\t 207\t [-0.22209993  0.22629368] \t1\tfalse\n",
            "(0)\t 208\t [ 0.5532299 -0.3676126] \t0\ttrue\n",
            "(1)\t 209\t [ 1.3759407 -1.3751544] \t0\tfalse\n",
            "(0)\t 210\t [ 2.1192732 -2.0763462] \t0\ttrue\n",
            "(1)\t 211\t [-0.14179192  0.04220167] \t1\ttrue\n",
            "(0)\t 212\t [ 1.6071317 -1.6410071] \t0\ttrue\n",
            "(1)\t 213\t [ 2.1661174 -2.127394 ] \t0\tfalse\n",
            "(0)\t 214\t [ 2.2907202 -2.2076752] \t0\ttrue\n",
            "(1)\t 215\t [ 2.0969381 -2.055222 ] \t0\tfalse\n",
            "(1)\t 216\t [ 2.0986834 -1.9057313] \t0\tfalse\n",
            "(0)\t 217\t [-1.4822872  1.2197895] \t1\tfalse\n",
            "(1)\t 218\t [ 0.95727324 -0.7441722 ] \t0\tfalse\n",
            "(0)\t 219\t [-1.2488189  1.0984714] \t1\tfalse\n",
            "(1)\t 220\t [-0.7005928  0.568371 ] \t1\ttrue\n",
            "(1)\t 221\t [ 1.7378428 -1.7848778] \t0\tfalse\n",
            "(1)\t 222\t [ 1.7378428 -1.7848778] \t0\tfalse\n",
            "(1)\t 223\t [-0.60023624  0.4907171 ] \t1\ttrue\n",
            "(0)\t 224\t [ 1.0540638 -1.2293404] \t0\ttrue\n",
            "(0)\t 225\t [ 1.7403692 -1.6397899] \t0\ttrue\n",
            "(0)\t 226\t [-1.3212138  1.104946 ] \t1\tfalse\n",
            "(1)\t 227\t [ 1.6284337 -1.4431307] \t0\tfalse\n",
            "(1)\t 228\t [ 1.6284337 -1.4431307] \t0\tfalse\n",
            "(1)\t 229\t [ 1.4133067 -1.2663776] \t0\tfalse\n",
            "(0)\t 230\t [ 2.016116 -2.02378 ] \t0\ttrue\n",
            "(1)\t 231\t [ 0.84784186 -0.80908614] \t0\tfalse\n",
            "(1)\t 232\t [ 0.84784186 -0.80908614] \t0\tfalse\n",
            "(0)\t 233\t [ 1.9804159 -1.8867507] \t0\ttrue\n",
            "(1)\t 234\t [ 1.2948822 -1.2234697] \t0\tfalse\n",
            "(0)\t 235\t [ 1.1893325  -0.93765414] \t0\ttrue\n",
            "(1)\t 236\t [ 2.3277857 -2.2945511] \t0\tfalse\n",
            "(1)\t 237\t [-2.022813   1.5922824] \t1\ttrue\n",
            "(0)\t 238\t [ 2.1458774 -2.0275204] \t0\ttrue\n",
            "(1)\t 239\t [ 1.9718881 -1.9289157] \t0\tfalse\n",
            "(0)\t 240\t [ 0.2295693 -0.3506702] \t0\ttrue\n",
            "(0)\t 241\t [ 1.0887492 -1.1584651] \t0\ttrue\n",
            "(1)\t 242\t [ 1.556261  -1.4466386] \t0\tfalse\n",
            "(1)\t 243\t [ 0.8224735  -0.55536616] \t0\tfalse\n",
            "(0)\t 244\t [ 1.8897568 -1.7760825] \t0\ttrue\n",
            "(1)\t 245\t [ 1.5295448 -1.3988492] \t0\tfalse\n",
            "(1)\t 246\t [ 2.2860897 -2.2209342] \t0\tfalse\n",
            "(1)\t 247\t [ 2.2860897 -2.2209342] \t0\tfalse\n",
            "(0)\t 248\t [ 2.014622  -1.9152138] \t0\ttrue\n",
            "(1)\t 249\t [ 0.22451378 -0.07713583] \t0\tfalse\n",
            "(1)\t 250\t [ 0.22451378 -0.07713583] \t0\tfalse\n",
            "(0)\t 251\t [ 1.6364226 -1.72415  ] \t0\ttrue\n",
            "(1)\t 252\t [ 1.9267824 -1.8998022] \t0\tfalse\n",
            "(1)\t 253\t [ 2.066005  -2.0125434] \t0\tfalse\n",
            "(1)\t 254\t [ 0.9215776 -0.9111407] \t0\tfalse\n",
            "(0)\t 255\t [ 1.520117  -1.5560902] \t0\ttrue\n",
            "(0)\t 256\t [-0.00554085  0.17054197] \t1\tfalse\n",
            "(1)\t 257\t [0.16443579 0.07900664] \t0\tfalse\n",
            "(0)\t 258\t [-0.05183119 -0.04395099] \t1\tfalse\n",
            "(0)\t 259\t [ 2.1596236 -2.1228113] \t0\ttrue\n",
            "(0)\t 260\t [ 2.254596  -2.1997428] \t0\ttrue\n",
            "(0)\t 261\t [ 2.2747805 -2.2672012] \t0\ttrue\n",
            "(0)\t 262\t [ 2.201553  -2.1349103] \t0\ttrue\n",
            "(0)\t 263\t [ 2.076619  -2.0847747] \t0\ttrue\n",
            "(0)\t 264\t [ 2.0081887 -1.7985637] \t0\ttrue\n",
            "(0)\t 265\t [ 2.2593615 -2.1884406] \t0\ttrue\n",
            "(1)\t 266\t [ 2.215513  -2.1353686] \t0\tfalse\n",
            "(0)\t 267\t [ 2.203793  -2.1046813] \t0\ttrue\n",
            "(0)\t 268\t [-1.774163   1.4318738] \t1\tfalse\n",
            "(0)\t 269\t [ 0.36666512 -0.5404408 ] \t0\ttrue\n",
            "(0)\t 270\t [ 0.9608153 -1.1279945] \t0\ttrue\n",
            "(1)\t 271\t [-0.2851948   0.27049562] \t1\ttrue\n",
            "(1)\t 272\t [-0.35031798  0.40599746] \t1\ttrue\n",
            "(1)\t 273\t [ 1.8710333 -1.7470844] \t0\tfalse\n",
            "(1)\t 274\t [ 0.23844561 -0.26499096] \t0\tfalse\n",
            "(1)\t 275\t [ 1.541715  -1.3594179] \t0\tfalse\n",
            "(1)\t 276\t [-1.789662   1.4976368] \t1\ttrue\n",
            "(1)\t 277\t [ 1.5774579 -1.5871319] \t0\tfalse\n",
            "(0)\t 278\t [ 2.1504085 -2.0138795] \t0\ttrue\n",
            "(0)\t 279\t [ 1.6408842 -1.8127763] \t0\ttrue\n",
            "(0)\t 280\t [ 1.904868  -1.7692324] \t0\ttrue\n",
            "(1)\t 281\t [-0.94679236  0.79725665] \t1\ttrue\n",
            "(0)\t 282\t [0.00734684 0.23273537] \t1\tfalse\n",
            "(1)\t 283\t [ 0.39557207 -0.47082835] \t0\tfalse\n",
            "(1)\t 284\t [ 1.4067068 -1.4189432] \t0\tfalse\n",
            "(1)\t 285\t [ 1.6166927 -1.5116271] \t0\tfalse\n",
            "(1)\t 286\t [ 1.6166927 -1.5116271] \t0\tfalse\n",
            "(1)\t 287\t [ 1.9295864 -1.8648763] \t0\tfalse\n",
            "(0)\t 288\t [ 1.701231  -1.4993166] \t0\ttrue\n",
            "(0)\t 289\t [ 2.2284424 -2.106825 ] \t0\ttrue\n",
            "(1)\t 290\t [-0.15595233  0.28388518] \t1\ttrue\n",
            "(1)\t 291\t [-1.5173012  1.2931392] \t1\ttrue\n",
            "(1)\t 292\t [ 0.48094317 -0.40619472] \t0\tfalse\n",
            "(1)\t 293\t [ 1.6810315 -1.5895872] \t0\tfalse\n",
            "(0)\t 294\t [ 2.033154  -2.0246277] \t0\ttrue\n",
            "(0)\t 295\t [ 1.6629325 -1.7005332] \t0\ttrue\n",
            "(0)\t 296\t [ 1.4722619 -1.6970317] \t0\ttrue\n",
            "(0)\t 297\t [-0.6289712  0.5603704] \t1\tfalse\n",
            "(0)\t 298\t [ 1.7426641 -1.8464197] \t0\ttrue\n",
            "(1)\t 299\t [ 0.4371568  -0.22241862] \t0\tfalse\n",
            "(0)\t 300\t [ 1.2540858 -1.3655126] \t0\ttrue\n",
            "(1)\t 301\t [-1.6112175  1.5076492] \t1\ttrue\n",
            "(1)\t 302\t [ 2.2898593 -2.2339618] \t0\tfalse\n",
            "(1)\t 303\t [ 2.2898593 -2.2339618] \t0\tfalse\n",
            "(1)\t 304\t [-2.0188932  1.6488242] \t1\ttrue\n",
            "(1)\t 305\t [-0.35642353  0.5612453 ] \t1\ttrue\n",
            "(1)\t 306\t [ 0.44270667 -0.3185284 ] \t0\tfalse\n",
            "(0)\t 307\t [ 2.203876 -2.115148] \t0\ttrue\n",
            "(1)\t 308\t [ 2.05575   -1.9058428] \t0\tfalse\n",
            "(0)\t 309\t [ 2.111439  -2.0771523] \t0\ttrue\n",
            "(0)\t 310\t [ 1.6010473 -1.8023751] \t0\ttrue\n",
            "(0)\t 311\t [-0.6050887   0.57056856] \t1\tfalse\n",
            "(0)\t 312\t [-0.03074048 -0.09756781] \t0\ttrue\n",
            "(0)\t 313\t [ 1.7129016 -1.8281932] \t0\ttrue\n",
            "(0)\t 314\t [ 1.7953938 -1.78232  ] \t0\ttrue\n",
            "(0)\t 315\t [ 2.0206277 -1.9010015] \t0\ttrue\n",
            "(0)\t 316\t [ 2.1216502 -2.1258943] \t0\ttrue\n",
            "(0)\t 317\t [ 2.2882733 -2.2084243] \t0\ttrue\n",
            "(1)\t 318\t [-1.7605367  1.4422262] \t1\ttrue\n",
            "(0)\t 319\t [ 1.4207244 -1.3787354] \t0\ttrue\n",
            "(1)\t 320\t [ 2.2360778 -2.1922045] \t0\tfalse\n",
            "(1)\t 321\t [ 2.1982281 -2.0929828] \t0\tfalse\n",
            "(0)\t 322\t [ 0.5126651 -0.7083123] \t0\ttrue\n",
            "(0)\t 323\t [ 2.2864068 -2.2163098] \t0\ttrue\n",
            "(0)\t 324\t [ 1.6233184 -1.4361084] \t0\ttrue\n",
            "(0)\t 325\t [ 2.1847036 -1.971199 ] \t0\ttrue\n",
            "(1)\t 326\t [ 1.8384588 -1.6835839] \t0\tfalse\n",
            "(0)\t 327\t [ 0.4188581 -0.5298706] \t0\ttrue\n",
            "(0)\t 328\t [ 2.037673  -2.0151415] \t0\ttrue\n",
            "(0)\t 329\t [-1.3642123  1.0467358] \t1\tfalse\n",
            "(1)\t 330\t [-1.8572267  1.6895648] \t1\ttrue\n",
            "(0)\t 331\t [ 0.91394156 -0.824594  ] \t0\ttrue\n",
            "(1)\t 332\t [-1.438642   1.2272942] \t1\ttrue\n",
            "(1)\t 333\t [-0.37611482  0.10696702] \t1\ttrue\n",
            "(1)\t 334\t [ 2.099682  -1.9610558] \t0\tfalse\n",
            "(0)\t 335\t [-1.1976366   0.99270636] \t1\tfalse\n",
            "(0)\t 336\t [ 1.7708755 -1.6939042] \t0\ttrue\n",
            "(0)\t 337\t [ 1.9206953 -1.7848101] \t0\ttrue\n",
            "(1)\t 338\t [ 1.7867787 -1.8917918] \t0\tfalse\n",
            "(1)\t 339\t [-1.1300182  0.8367635] \t1\ttrue\n",
            "(1)\t 340\t [-1.93061    1.6216145] \t1\ttrue\n",
            "(0)\t 341\t [ 0.27467948 -0.4864872 ] \t0\ttrue\n",
            "(1)\t 342\t [ 1.465785  -1.6415334] \t0\tfalse\n",
            "(1)\t 343\t [ 0.33011863 -0.10899365] \t0\tfalse\n",
            "(0)\t 344\t [-1.4076021  1.3032401] \t1\tfalse\n",
            "(1)\t 345\t [ 1.6348193 -1.2954465] \t0\tfalse\n",
            "(1)\t 346\t [ 1.702169  -1.5916502] \t0\tfalse\n",
            "(0)\t 347\t [ 2.0278046 -2.070955 ] \t0\ttrue\n",
            "(0)\t 348\t [-0.57441676  0.28459144] \t1\tfalse\n",
            "(0)\t 349\t [ 0.9106649 -1.0424377] \t0\ttrue\n",
            "(0)\t 350\t [ 1.6648782 -1.6359563] \t0\ttrue\n",
            "(0)\t 351\t [-0.4369072   0.26896673] \t1\tfalse\n",
            "(0)\t 352\t [ 2.1578157 -2.0137353] \t0\ttrue\n",
            "(1)\t 353\t [-0.7326158  0.9258013] \t1\ttrue\n",
            "(0)\t 354\t [ 1.8548844 -1.8781155] \t0\ttrue\n",
            "(0)\t 355\t [ 1.6309745 -1.4722065] \t0\ttrue\n",
            "(0)\t 356\t [ 1.9096086 -1.870606 ] \t0\ttrue\n",
            "(0)\t 357\t [ 1.6043392 -1.7832175] \t0\ttrue\n",
            "(0)\t 358\t [ 2.1102705 -2.1007624] \t0\ttrue\n",
            "(0)\t 359\t [ 2.2959723 -2.1809304] \t0\ttrue\n",
            "(0)\t 360\t [ 1.2264453 -1.3217607] \t0\ttrue\n",
            "(0)\t 361\t [ 1.4845978 -1.5130129] \t0\ttrue\n",
            "(0)\t 362\t [ 1.5328906 -1.4307656] \t0\ttrue\n",
            "(0)\t 363\t [ 1.6656477 -1.520869 ] \t0\ttrue\n",
            "(0)\t 364\t [ 0.91503    -0.75054353] \t0\ttrue\n",
            "(0)\t 365\t [ 0.73128784 -1.0174206 ] \t0\ttrue\n",
            "(0)\t 366\t [ 2.2885492 -2.192111 ] \t0\ttrue\n",
            "(1)\t 367\t [ 1.0097477 -1.1870272] \t0\tfalse\n",
            "(0)\t 368\t [ 1.9001216 -1.9145589] \t0\ttrue\n",
            "(0)\t 369\t [ 2.2830455 -2.1708448] \t0\ttrue\n",
            "(0)\t 370\t [-0.31229883  0.22644728] \t1\tfalse\n",
            "(0)\t 371\t [ 1.3900657 -1.1818911] \t0\ttrue\n",
            "(0)\t 372\t [-1.9294019  1.7055238] \t1\tfalse\n",
            "(0)\t 373\t [ 1.5711389 -1.6485431] \t0\ttrue\n",
            "(0)\t 374\t [-1.5884839  1.205969 ] \t1\tfalse\n",
            "(0)\t 375\t [ 2.128665  -2.0073938] \t0\ttrue\n",
            "(0)\t 376\t [ 2.2938561 -2.1944067] \t0\ttrue\n",
            "(0)\t 377\t [ 2.234129  -2.0989816] \t0\ttrue\n",
            "(0)\t 378\t [ 0.72590077 -0.55026525] \t0\ttrue\n",
            "(0)\t 379\t [-0.17631127 -0.0915411 ] \t1\tfalse\n",
            "(1)\t 380\t [ 1.9842268 -1.957665 ] \t0\tfalse\n",
            "(0)\t 381\t [-1.5876448  1.3709669] \t1\tfalse\n",
            "(0)\t 382\t [ 1.75591   -1.6040057] \t0\ttrue\n",
            "(0)\t 383\t [ 2.128484  -1.8826474] \t0\ttrue\n",
            "(1)\t 384\t [ 1.5803483 -1.751878 ] \t0\tfalse\n",
            "(0)\t 385\t [ 2.1504943 -2.1050332] \t0\ttrue\n",
            "(0)\t 386\t [ 0.8164653  -0.67759293] \t0\ttrue\n",
            "(0)\t 387\t [ 0.89564073 -1.0803977 ] \t0\ttrue\n",
            "(1)\t 388\t [-1.3894931  1.338099 ] \t1\ttrue\n",
            "(0)\t 389\t [ 2.2271893 -2.1010115] \t0\ttrue\n",
            "(0)\t 390\t [ 2.176694  -2.1658552] \t0\ttrue\n",
            "(0)\t 391\t [ 2.157015  -2.0776155] \t0\ttrue\n",
            "(0)\t 392\t [ 1.998347  -1.9014882] \t0\ttrue\n",
            "(0)\t 393\t [-1.6586885  1.3498943] \t1\tfalse\n",
            "(0)\t 394\t [ 1.377991  -1.3211467] \t0\ttrue\n",
            "(1)\t 395\t [ 0.5768106  -0.38861144] \t0\tfalse\n",
            "(0)\t 396\t [-1.3020681  1.1286241] \t1\tfalse\n",
            "(0)\t 397\t [ 1.3868725 -1.5176772] \t0\ttrue\n",
            "(0)\t 398\t [-1.7252885  1.5345172] \t1\tfalse\n",
            "(0)\t 399\t [ 1.9513997 -1.8678179] \t0\ttrue\n",
            "(0)\t 400\t [ 2.1945007 -2.2214777] \t0\ttrue\n",
            "(0)\t 401\t [ 1.9361075 -1.9098123] \t0\ttrue\n",
            "(0)\t 402\t [ 1.5593269 -1.5652566] \t0\ttrue\n",
            "(0)\t 403\t [-0.60058314  0.60056686] \t1\tfalse\n",
            "(0)\t 404\t [-1.1113718   0.93280256] \t1\tfalse\n",
            "(0)\t 405\t [-0.4139912  0.404907 ] \t1\tfalse\n",
            "(0)\t 406\t [ 2.093119  -2.0736287] \t0\ttrue\n",
            "(0)\t 407\t [-0.01694423  0.18567309] \t1\tfalse\n",
            "(1)\t 408\t [ 0.33102947 -0.3377229 ] \t0\tfalse\n",
            "(0)\t 409\t [ 1.8528315 -1.9224732] \t0\ttrue\n",
            "(0)\t 410\t [ 1.4162884 -1.1640036] \t0\ttrue\n",
            "(0)\t 411\t [ 1.2671882 -1.3198738] \t0\ttrue\n",
            "(0)\t 412\t [ 2.1251493 -2.1034534] \t0\ttrue\n",
            "(0)\t 413\t [ 1.1345773 -1.1243794] \t0\ttrue\n",
            "(0)\t 414\t [-1.8533825  1.6459514] \t1\tfalse\n",
            "(0)\t 415\t [ 1.1690042 -1.3137698] \t0\ttrue\n",
            "(1)\t 416\t [ 0.8713096  -0.74715716] \t0\tfalse\n",
            "(0)\t 417\t [ 2.2849538 -2.1934657] \t0\ttrue\n",
            "(1)\t 418\t [ 0.94390416 -1.1108868 ] \t0\tfalse\n",
            "(0)\t 419\t [ 2.0854104 -2.1505427] \t0\ttrue\n",
            "(1)\t 420\t [-0.21877685  0.18614608] \t1\ttrue\n",
            "(0)\t 421\t [ 1.5209224 -1.5720961] \t0\ttrue\n",
            "(0)\t 422\t [ 0.95148903 -0.78857267] \t0\ttrue\n",
            "(1)\t 423\t [ 2.0509584 -2.0541558] \t0\tfalse\n",
            "(0)\t 424\t [ 1.9074032 -1.935049 ] \t0\ttrue\n",
            "(0)\t 425\t [ 0.47465944 -0.42552245] \t0\ttrue\n",
            "(1)\t 426\t [-1.8102558  1.4401499] \t1\ttrue\n",
            "(0)\t 427\t [ 1.5788676 -1.5657551] \t0\ttrue\n",
            "(0)\t 428\t [ 1.3838823 -1.2404606] \t0\ttrue\n",
            "(0)\t 429\t [ 1.5987892 -1.7076077] \t0\ttrue\n",
            "(0)\t 430\t [ 2.2905247 -2.2333505] \t0\ttrue\n",
            "(1)\t 431\t [-0.6014606   0.60969085] \t1\ttrue\n",
            "(1)\t 432\t [ 2.147639  -2.1633046] \t0\tfalse\n",
            "(1)\t 433\t [ 2.147639  -2.1633046] \t0\tfalse\n",
            "(0)\t 434\t [ 0.99721265 -1.1183704 ] \t0\ttrue\n",
            "(1)\t 435\t [ 2.2811167 -2.2091873] \t0\tfalse\n",
            "(1)\t 436\t [ 2.2811167 -2.2091873] \t0\tfalse\n",
            "(0)\t 437\t [ 2.1903343 -2.1324322] \t0\ttrue\n",
            "(0)\t 438\t [-1.5392392  1.3245763] \t1\tfalse\n",
            "(0)\t 439\t [-1.4589329  1.2425122] \t1\tfalse\n",
            "(0)\t 440\t [ 1.1019366 -0.8984352] \t0\ttrue\n",
            "(0)\t 441\t [ 1.9618083 -1.8741527] \t0\ttrue\n",
            "(1)\t 442\t [ 1.380757  -1.1636374] \t0\tfalse\n",
            "(1)\t 443\t [ 2.147963 -2.106489] \t0\tfalse\n",
            "(0)\t 444\t [ 1.8363738 -2.0172186] \t0\ttrue\n",
            "(0)\t 445\t [ 0.6774486  -0.71940255] \t0\ttrue\n",
            "(1)\t 446\t [ 1.8184885 -1.9538736] \t0\tfalse\n",
            "(0)\t 447\t [ 2.2270977 -2.1309392] \t0\ttrue\n",
            "(0)\t 448\t [ 1.2297124 -1.2044295] \t0\ttrue\n",
            "(0)\t 449\t [ 1.4827536 -1.2483478] \t0\ttrue\n",
            "(1)\t 450\t [ 1.4388561 -1.3654226] \t0\tfalse\n",
            "(1)\t 451\t [ 1.4388561 -1.3654226] \t0\tfalse\n",
            "(1)\t 452\t [ 2.2242434 -2.2177894] \t0\tfalse\n",
            "(0)\t 453\t [ 1.8029263 -1.8226658] \t0\ttrue\n",
            "(0)\t 454\t [ 2.0448363 -1.9911081] \t0\ttrue\n",
            "(0)\t 455\t [ 0.7005793 -0.5264816] \t0\ttrue\n",
            "(1)\t 456\t [-0.7951593   0.81933963] \t1\ttrue\n",
            "(1)\t 457\t [ 0.9892669 -1.1109085] \t0\tfalse\n",
            "(1)\t 458\t [ 2.011114 -2.033001] \t0\tfalse\n",
            "(0)\t 459\t [ 1.8396273 -1.7882956] \t0\ttrue\n",
            "(1)\t 460\t [ 2.0921197 -2.1281192] \t0\tfalse\n",
            "(1)\t 461\t [ 2.0921197 -2.1281192] \t0\tfalse\n",
            "(0)\t 462\t [ 2.20156   -2.1045763] \t0\ttrue\n",
            "(1)\t 463\t [ 2.2420533 -2.1378186] \t0\tfalse\n",
            "(1)\t 464\t [ 1.4213896 -1.3463428] \t0\tfalse\n",
            "(1)\t 465\t [ 2.2517037 -2.1277335] \t0\tfalse\n",
            "(1)\t 466\t [-1.7785248  1.4095032] \t1\ttrue\n",
            "(0)\t 467\t [ 1.9558644 -2.0408185] \t0\ttrue\n",
            "(0)\t 468\t [ 1.4819288 -1.4726759] \t0\ttrue\n",
            "(1)\t 469\t [ 0.5239912 -0.295768 ] \t0\tfalse\n",
            "(1)\t 470\t [ 1.846576  -1.7568779] \t0\tfalse\n",
            "(1)\t 471\t [ 2.1025379 -2.1590078] \t0\tfalse\n",
            "(1)\t 472\t [ 2.1025379 -2.1590078] \t0\tfalse\n",
            "(0)\t 473\t [ 0.23816209 -0.41922432] \t0\ttrue\n",
            "(1)\t 474\t [-0.12328395  0.29367757] \t1\ttrue\n",
            "(0)\t 475\t [ 2.0022614 -1.9574978] \t0\ttrue\n",
            "(0)\t 476\t [ 1.9294088 -2.0823236] \t0\ttrue\n",
            "(0)\t 477\t [ 2.1351984 -2.1133976] \t0\ttrue\n",
            "(0)\t 478\t [ 1.6718017 -1.8211174] \t0\ttrue\n",
            "(1)\t 479\t [ 0.97354907 -0.82379544] \t0\tfalse\n",
            "(1)\t 480\t [ 1.4934916 -1.5620611] \t0\tfalse\n",
            "(1)\t 481\t [ 1.9877096 -1.9977612] \t0\tfalse\n",
            "(1)\t 482\t [ 1.9101954 -1.9210718] \t0\tfalse\n",
            "(0)\t 483\t [ 2.1687925 -2.0805614] \t0\ttrue\n",
            "(1)\t 484\t [ 1.5994487 -1.4229724] \t0\tfalse\n",
            "(0)\t 485\t [ 2.139738  -2.1754405] \t0\ttrue\n",
            "(1)\t 486\t [ 1.6718147 -1.7730522] \t0\tfalse\n",
            "(1)\t 487\t [ 1.8776233 -1.9291544] \t0\tfalse\n",
            "(0)\t 488\t [ 1.8662503 -1.8376105] \t0\ttrue\n",
            "(0)\t 489\t [ 1.4522307 -1.3336877] \t0\ttrue\n",
            "(0)\t 490\t [ 2.3025844 -2.1739223] \t0\ttrue\n",
            "(0)\t 491\t [ 1.6336942 -1.7653075] \t0\ttrue\n",
            "(1)\t 492\t [ 2.122979  -2.0140197] \t0\tfalse\n",
            "(1)\t 493\t [ 2.122979  -2.0140197] \t0\tfalse\n",
            "(0)\t 494\t [ 1.0601953 -1.2233965] \t0\ttrue\n",
            "(0)\t 495\t [ 2.1157982 -1.8926909] \t0\ttrue\n",
            "(1)\t 496\t [ 2.1236682 -2.0525804] \t0\tfalse\n",
            "(1)\t 497\t [-0.3897104   0.35640085] \t1\ttrue\n",
            "(0)\t 498\t [ 1.2417086 -1.1134273] \t0\ttrue\n",
            "(0)\t 499\t [ 2.3513095 -2.226117 ] \t0\ttrue\n",
            "(0)\t 500\t [0.18253697 0.09872645] \t0\ttrue\n",
            "(1)\t 501\t [ 2.234237 -2.141873] \t0\tfalse\n",
            "(1)\t 502\t [ 2.234237 -2.141873] \t0\tfalse\n",
            "(0)\t 503\t [ 1.9300029 -1.7783328] \t0\ttrue\n",
            "(0)\t 504\t [ 2.2418807 -2.2044904] \t0\ttrue\n",
            "(1)\t 505\t [-0.8764962  0.6563935] \t1\ttrue\n",
            "(1)\t 506\t [ 2.085815  -1.9433553] \t0\tfalse\n",
            "(1)\t 507\t [ 0.73081994 -0.7193418 ] \t0\tfalse\n",
            "(0)\t 508\t [-1.6468817  1.3561566] \t1\tfalse\n",
            "(0)\t 509\t [-1.3539643  1.0061345] \t1\tfalse\n",
            "(0)\t 510\t [ 1.117438 -1.099665] \t0\ttrue\n",
            "(1)\t 511\t [-1.6049033  1.284209 ] \t1\ttrue\n",
            "(1)\t 512\t [ 1.6597272 -1.538146 ] \t0\tfalse\n",
            "(0)\t 513\t [ 1.6789482 -1.8125614] \t0\ttrue\n",
            "(0)\t 514\t [-0.5752051   0.42008275] \t1\tfalse\n",
            "(1)\t 515\t [-2.0522296  1.6206003] \t1\ttrue\n",
            "(0)\t 516\t [ 2.0342739 -1.9771557] \t0\ttrue\n",
            "(1)\t 517\t [ 0.5669595 -0.4126262] \t0\tfalse\n",
            "(0)\t 518\t [ 2.2478163 -2.127165 ] \t0\ttrue\n",
            "(0)\t 519\t [ 1.756562  -1.6214805] \t0\ttrue\n",
            "(0)\t 520\t [ 2.0529494 -2.1077518] \t0\ttrue\n",
            "(0)\t 521\t [ 0.20790088 -0.03146779] \t0\ttrue\n",
            "(1)\t 522\t [ 1.0566006  -0.95206517] \t0\tfalse\n",
            "(1)\t 523\t [ 1.4639096 -1.4587818] \t0\tfalse\n",
            "(1)\t 524\t [ 0.9632022 -1.0765057] \t0\tfalse\n",
            "(1)\t 525\t [ 1.6095136 -1.480118 ] \t0\tfalse\n",
            "(1)\t 526\t [ 1.0518523 -1.017735 ] \t0\tfalse\n",
            "(1)\t 527\t [-1.0626733  1.1586152] \t1\ttrue\n",
            "(0)\t 528\t [ 2.0458486 -2.092711 ] \t0\ttrue\n",
            "(1)\t 529\t [ 1.0572716 -1.0880469] \t0\tfalse\n",
            "(1)\t 530\t [ 1.0572716 -1.0880469] \t0\tfalse\n",
            "(1)\t 531\t [ 0.6545619  -0.68644726] \t0\tfalse\n",
            "(1)\t 532\t [-1.2100487  0.9331772] \t1\ttrue\n",
            "(1)\t 533\t [ 2.2402854 -2.2515705] \t0\tfalse\n",
            "(1)\t 534\t [ 2.2402854 -2.2515705] \t0\tfalse\n",
            "(1)\t 535\t [ 1.3525712 -1.2928946] \t0\tfalse\n",
            "(1)\t 536\t [ 1.3525712 -1.2928946] \t0\tfalse\n",
            "(1)\t 537\t [ 2.284391  -2.2170134] \t0\tfalse\n",
            "(1)\t 538\t [-0.8395756   0.53977656] \t1\ttrue\n",
            "(1)\t 539\t [-0.8395756   0.53977656] \t1\ttrue\n",
            "(1)\t 540\t [-1.9191176  1.4696043] \t1\ttrue\n",
            "(1)\t 541\t [-1.9191176  1.4696043] \t1\ttrue\n",
            "(0)\t 542\t [ 0.595681   -0.42939788] \t0\ttrue\n",
            "(1)\t 543\t [ 2.084535 -2.085058] \t0\tfalse\n",
            "(1)\t 544\t [ 2.084535 -2.085058] \t0\tfalse\n",
            "(1)\t 545\t [ 1.8491127 -1.7222888] \t0\tfalse\n",
            "(1)\t 546\t [ 1.4896365 -1.312366 ] \t0\tfalse\n",
            "(0)\t 547\t [-0.8447413  0.662535 ] \t1\tfalse\n",
            "(1)\t 548\t [ 2.0896928 -2.0183897] \t0\tfalse\n",
            "(1)\t 549\t [ 2.0896928 -2.0183897] \t0\tfalse\n",
            "(0)\t 550\t [ 2.2486918 -2.2130458] \t0\ttrue\n",
            "(0)\t 551\t [ 0.4664725 -0.6848062] \t0\ttrue\n",
            "(0)\t 552\t [ 1.8010626 -1.7279687] \t0\ttrue\n",
            "(1)\t 553\t [-0.6638597  0.3453333] \t1\ttrue\n",
            "(0)\t 554\t [ 2.0980158 -2.0205376] \t0\ttrue\n",
            "(0)\t 555\t [ 1.4526393 -1.7545574] \t0\ttrue\n",
            "(1)\t 556\t [-0.6871226   0.43484622] \t1\ttrue\n",
            "(1)\t 557\t [-2.0232856  1.604589 ] \t1\ttrue\n",
            "(1)\t 558\t [ 0.8124588 -0.6035766] \t0\tfalse\n",
            "(0)\t 559\t [-0.39275163  0.6654214 ] \t1\tfalse\n",
            "(0)\t 560\t [-0.29577872  0.2601633 ] \t1\tfalse\n",
            "(0)\t 561\t [ 0.2974294  -0.37990373] \t0\ttrue\n",
            "(1)\t 562\t [ 2.1454854 -2.1747377] \t0\tfalse\n",
            "(1)\t 563\t [ 2.1454854 -2.1747377] \t0\tfalse\n",
            "(1)\t 564\t [ 1.773416  -1.6940209] \t0\tfalse\n",
            "(0)\t 565\t [ 1.0706415  -0.86314166] \t0\ttrue\n",
            "(1)\t 566\t [ 0.5650376  -0.75798213] \t0\tfalse\n",
            "(1)\t 567\t [ 0.5650376  -0.75798213] \t0\tfalse\n",
            "(0)\t 568\t [ 1.9250747 -1.9518957] \t0\ttrue\n",
            "(0)\t 569\t [ 0.918975  -1.2634423] \t0\ttrue\n",
            "(0)\t 570\t [ 2.111779 -2.11788 ] \t0\ttrue\n",
            "(0)\t 571\t [ 1.9153844 -1.8190469] \t0\ttrue\n",
            "(1)\t 572\t [ 0.82292134 -0.9694925 ] \t0\tfalse\n",
            "(1)\t 573\t [ 1.704104  -1.8079652] \t0\tfalse\n",
            "(0)\t 574\t [ 1.5492892 -1.7786453] \t0\ttrue\n",
            "(0)\t 575\t [ 0.19173232 -0.07711412] \t0\ttrue\n",
            "(1)\t 576\t [ 2.239137 -2.12413 ] \t0\tfalse\n",
            "(0)\t 577\t [-1.3488731  1.0592844] \t1\tfalse\n",
            "(0)\t 578\t [ 2.2336984 -2.1989725] \t0\ttrue\n",
            "(0)\t 579\t [-0.6851663  0.4872325] \t1\tfalse\n",
            "(0)\t 580\t [ 2.2610269 -2.1844933] \t0\ttrue\n",
            "(1)\t 581\t [ 1.3969374 -1.4483266] \t0\tfalse\n",
            "(1)\t 582\t [ 0.34955826 -0.14265326] \t0\tfalse\n",
            "(1)\t 583\t [ 0.34955826 -0.14265326] \t0\tfalse\n",
            "(0)\t 584\t [ 1.790935  -1.8178995] \t0\ttrue\n",
            "(1)\t 585\t [ 1.5907626 -1.5330913] \t0\tfalse\n",
            "(0)\t 586\t [ 2.3283885 -2.2210119] \t0\ttrue\n",
            "(1)\t 587\t [ 2.110593  -2.0348942] \t0\tfalse\n",
            "(0)\t 588\t [ 2.0283685 -2.0723298] \t0\ttrue\n",
            "(1)\t 589\t [ 0.42711586 -0.46377188] \t0\tfalse\n",
            "(1)\t 590\t [ 0.77995116 -0.7882955 ] \t0\tfalse\n",
            "(1)\t 591\t [ 1.6579311 -1.7393862] \t0\tfalse\n",
            "(0)\t 592\t [ 2.0868483 -2.0162666] \t0\ttrue\n",
            "(1)\t 593\t [ 1.9255306 -2.0717034] \t0\tfalse\n",
            "(1)\t 594\t [ 1.9255306 -2.0717034] \t0\tfalse\n",
            "(0)\t 595\t [ 2.0397282 -1.9616321] \t0\ttrue\n",
            "(0)\t 596\t [ 1.8548198 -1.7461398] \t0\ttrue\n",
            "(1)\t 597\t [-1.9180095  1.6085292] \t1\ttrue\n",
            "(0)\t 598\t [ 0.17659494 -0.40902865] \t0\ttrue\n",
            "(0)\t 599\t [-0.8189179   0.87419415] \t1\tfalse\n",
            "(0)\t 600\t [-1.4543009  1.209579 ] \t1\tfalse\n",
            "(0)\t 601\t [ 1.739582  -1.8801217] \t0\ttrue\n",
            "(0)\t 602\t [ 1.5583308 -1.728944 ] \t0\ttrue\n",
            "(1)\t 603\t [ 1.4079292 -1.2852473] \t0\tfalse\n",
            "(1)\t 604\t [ 2.018996  -1.8104432] \t0\tfalse\n",
            "(1)\t 605\t [ 2.018996  -1.8104432] \t0\tfalse\n",
            "(1)\t 606\t [ 2.25678   -2.2609055] \t0\tfalse\n",
            "(0)\t 607\t [ 2.0545685 -2.0171812] \t0\ttrue\n",
            "(0)\t 608\t [ 1.4480591 -1.2485712] \t0\ttrue\n",
            "(1)\t 609\t [ 0.5273185  -0.41655552] \t0\tfalse\n",
            "(1)\t 610\t [ 0.5273185  -0.41655552] \t0\tfalse\n",
            "(0)\t 611\t [ 1.3620074 -1.5472192] \t0\ttrue\n",
            "(1)\t 612\t [ 1.0855854  -0.95649475] \t0\tfalse\n",
            "(1)\t 613\t [ 2.1644852 -2.019564 ] \t0\tfalse\n",
            "(0)\t 614\t [ 1.7060187 -1.5380378] \t0\ttrue\n",
            "(0)\t 615\t [ 0.65694857 -0.7407193 ] \t0\ttrue\n",
            "(0)\t 616\t [ 1.228934  -1.1962306] \t0\ttrue\n",
            "(0)\t 617\t [-0.7369472  0.5281991] \t1\tfalse\n",
            "(0)\t 618\t [ 1.6682942 -1.5310283] \t0\ttrue\n",
            "(1)\t 619\t [ 1.8284489 -1.8447418] \t0\tfalse\n",
            "(0)\t 620\t [ 1.4352099 -1.1718107] \t0\ttrue\n",
            "(1)\t 621\t [ 1.4696379 -1.3649662] \t0\tfalse\n",
            "(1)\t 622\t [ 1.4696379 -1.3649662] \t0\tfalse\n",
            "(0)\t 623\t [ 1.0361782 -0.7794952] \t0\ttrue\n",
            "(0)\t 624\t [ 1.8176501 -1.9585854] \t0\ttrue\n",
            "(0)\t 625\t [ 1.0257514  -0.84499633] \t0\ttrue\n",
            "(0)\t 626\t [ 1.3778446 -1.4818659] \t0\ttrue\n",
            "(0)\t 627\t [ 2.2179997 -2.1916254] \t0\ttrue\n",
            "(1)\t 628\t [-1.8841821  1.5989158] \t1\ttrue\n",
            "(0)\t 629\t [ 1.753629  -1.6581881] \t0\ttrue\n",
            "(1)\t 630\t [-0.2948527   0.20520851] \t1\ttrue\n",
            "(0)\t 631\t [-0.08855872 -0.05885332] \t1\tfalse\n",
            "(1)\t 632\t [ 2.193282 -2.117336] \t0\tfalse\n",
            "(0)\t 633\t [ 1.5813049 -1.4295304] \t0\ttrue\n",
            "(1)\t 634\t [ 0.96400416 -0.8305708 ] \t0\tfalse\n",
            "(1)\t 635\t [ 1.2501502 -1.1916473] \t0\tfalse\n",
            "(1)\t 636\t [ 2.1252294 -2.0948799] \t0\tfalse\n",
            "(1)\t 637\t [ 1.6310263 -1.5320446] \t0\tfalse\n",
            "(1)\t 638\t [-1.5253989  1.3541996] \t1\ttrue\n",
            "(0)\t 639\t [ 0.7553388 -0.8736495] \t0\ttrue\n",
            "(1)\t 640\t [-2.0172672  1.6224424] \t1\ttrue\n",
            "(1)\t 641\t [ 1.4398471 -1.3956411] \t0\tfalse\n",
            "(0)\t 642\t [ 2.0818198 -2.0892537] \t0\ttrue\n",
            "(1)\t 643\t [ 0.93457764 -0.7238177 ] \t0\tfalse\n",
            "(1)\t 644\t [ 0.14159457 -0.03076263] \t0\tfalse\n",
            "(1)\t 645\t [-0.7319381   0.91294396] \t1\ttrue\n",
            "(0)\t 646\t [ 0.25060093 -0.35454437] \t0\ttrue\n",
            "(1)\t 647\t [-0.01919649 -0.23887727] \t0\tfalse\n",
            "(0)\t 648\t [ 1.8393662 -1.947401 ] \t0\ttrue\n",
            "(1)\t 649\t [-1.3452584  1.1429837] \t1\ttrue\n",
            "(0)\t 650\t [-1.3755789  1.3049905] \t1\tfalse\n",
            "(0)\t 651\t [ 2.0808449 -1.9677329] \t0\ttrue\n",
            "(1)\t 652\t [-0.94415975  0.7338327 ] \t1\ttrue\n",
            "(1)\t 653\t [ 1.2871926 -1.1270324] \t0\tfalse\n",
            "(0)\t 654\t [-1.642181   1.4145937] \t1\tfalse\n",
            "(1)\t 655\t [ 1.4742197 -1.568076 ] \t0\tfalse\n",
            "(1)\t 656\t [ 1.3936763 -1.5123165] \t0\tfalse\n",
            "(0)\t 657\t [ 2.2659075 -2.193213 ] \t0\ttrue\n",
            "(0)\t 658\t [ 2.2121205 -2.0973725] \t0\ttrue\n",
            "(0)\t 659\t [ 1.794833  -1.6423154] \t0\ttrue\n",
            "(0)\t 660\t [ 2.1875522 -2.17039  ] \t0\ttrue\n",
            "(0)\t 661\t [ 1.7125129 -1.7260084] \t0\ttrue\n",
            "(1)\t 662\t [-0.29606432  0.14823891] \t1\ttrue\n",
            "(1)\t 663\t [ 0.45408803 -0.35491443] \t0\tfalse\n",
            "(1)\t 664\t [ 0.45408803 -0.35491443] \t0\tfalse\n",
            "(0)\t 665\t [ 2.2484443 -2.1948717] \t0\ttrue\n",
            "(1)\t 666\t [ 1.270611  -1.1136787] \t0\tfalse\n",
            "(0)\t 667\t [ 1.7680731 -1.9067241] \t0\ttrue\n",
            "(1)\t 668\t [ 0.8506668  -0.90089417] \t0\tfalse\n",
            "(0)\t 669\t [ 1.5929261 -1.446251 ] \t0\ttrue\n",
            "(0)\t 670\t [ 1.9955032 -2.0523174] \t0\ttrue\n",
            "(0)\t 671\t [ 2.244408  -2.1877897] \t0\ttrue\n",
            "(0)\t 672\t [ 1.934521  -1.9499345] \t0\ttrue\n",
            "(1)\t 673\t [ 2.0661514 -1.9851935] \t0\tfalse\n",
            "(1)\t 674\t [ 1.9644282 -1.8955748] \t0\tfalse\n",
            "(1)\t 675\t [ 1.9644282 -1.8955748] \t0\tfalse\n",
            "(1)\t 676\t [ 1.6990504 -1.5196059] \t0\tfalse\n",
            "(1)\t 677\t [ 1.9282529 -1.9280666] \t0\tfalse\n",
            "(1)\t 678\t [-0.51623935  0.44409764] \t1\ttrue\n",
            "(0)\t 679\t [-1.7088809  1.3813353] \t1\tfalse\n",
            "(1)\t 680\t [ 0.15739492 -0.00591059] \t0\tfalse\n",
            "(0)\t 681\t [ 2.2518554 -2.1230626] \t0\ttrue\n",
            "(0)\t 682\t [ 2.1566215 -2.067655 ] \t0\ttrue\n",
            "(0)\t 683\t [ 0.63998437 -0.8465016 ] \t0\ttrue\n",
            "(0)\t 684\t [-1.69473   1.438894] \t1\tfalse\n",
            "(1)\t 685\t [-0.6885399   0.50430626] \t1\ttrue\n",
            "(1)\t 686\t [ 2.0239785 -1.826621 ] \t0\tfalse\n",
            "(1)\t 687\t [ 2.0811527 -2.0399122] \t0\tfalse\n",
            "(1)\t 688\t [ 2.178005  -2.0664747] \t0\tfalse\n",
            "(0)\t 689\t [ 2.1912353 -2.1293073] \t0\ttrue\n",
            "(1)\t 690\t [ 1.4801893 -1.3494418] \t0\tfalse\n",
            "(1)\t 691\t [-0.07106775 -0.12839028] \t0\tfalse\n",
            "(0)\t 692\t [ 2.3520525 -2.2139776] \t0\ttrue\n",
            "(0)\t 693\t [ 1.3611217 -1.1795118] \t0\ttrue\n",
            "(1)\t 694\t [ 2.108551  -2.1302032] \t0\tfalse\n",
            "(1)\t 695\t [ 1.7617383 -1.6891026] \t0\tfalse\n",
            "(0)\t 696\t [ 2.3360608 -2.2752678] \t0\ttrue\n",
            "(0)\t 697\t [ 1.1996559 -1.101444 ] \t0\ttrue\n",
            "(0)\t 698\t [ 0.9619223 -0.9406247] \t0\ttrue\n",
            "(0)\t 699\t [ 0.9172103 -0.9786327] \t0\ttrue\n",
            "(0)\t 700\t [ 2.1069555 -2.1133134] \t0\ttrue\n",
            "(0)\t 701\t [ 1.7453206 -1.9058129] \t0\ttrue\n",
            "(0)\t 702\t [ 1.2929466 -1.2437158] \t0\ttrue\n",
            "(0)\t 703\t [ 2.2050397 -2.12906  ] \t0\ttrue\n",
            "(0)\t 704\t [ 0.9586595 -1.1322262] \t0\ttrue\n",
            "(1)\t 705\t [ 1.5671152 -1.4236646] \t0\tfalse\n",
            "(0)\t 706\t [-0.80366844  0.71706986] \t1\tfalse\n",
            "(0)\t 707\t [ 2.2542388 -2.220609 ] \t0\ttrue\n",
            "(1)\t 708\t [ 1.3277274 -1.244596 ] \t0\tfalse\n",
            "(0)\t 709\t [-1.7239301  1.3995528] \t1\tfalse\n",
            "(0)\t 710\t [ 1.7334301 -1.9084747] \t0\ttrue\n",
            "(1)\t 711\t [ 2.169612  -2.0809824] \t0\tfalse\n",
            "(1)\t 712\t [-0.8043874  0.6634468] \t1\ttrue\n",
            "(1)\t 713\t [ 0.95075834 -0.88238645] \t0\tfalse\n",
            "(1)\t 714\t [ 0.6420895 -0.5453943] \t0\tfalse\n",
            "(1)\t 715\t [-0.83872414  0.841262  ] \t1\ttrue\n",
            "(0)\t 716\t [ 1.1258719 -1.0436274] \t0\ttrue\n",
            "(0)\t 717\t [ 2.006806  -2.0372584] \t0\ttrue\n",
            "(0)\t 718\t [ 0.66370595 -0.8813741 ] \t0\ttrue\n",
            "(0)\t 719\t [ 2.1437376 -2.1687343] \t0\ttrue\n",
            "(0)\t 720\t [ 2.1170654 -2.1737409] \t0\ttrue\n",
            "(1)\t 721\t [ 1.8224213 -1.7090861] \t0\tfalse\n",
            "(1)\t 722\t [-1.8156819  1.5396237] \t1\ttrue\n",
            "(0)\t 723\t [ 1.8332201 -1.9122145] \t0\ttrue\n",
            "(0)\t 724\t [ 1.6530237 -1.5369446] \t0\ttrue\n",
            "(1)\t 725\t [ 2.3180408 -2.2033308] \t0\tfalse\n",
            "(0)\t 726\t [ 1.6190903 -1.4979408] \t0\ttrue\n",
            "(0)\t 727\t [ 2.130579 -2.034328] \t0\ttrue\n",
            "(1)\t 728\t [-2.1030636  1.7671874] \t1\ttrue\n",
            "(1)\t 729\t [ 2.0306697 -1.9249191] \t0\tfalse\n",
            "(1)\t 730\t [-1.9603914  1.7094584] \t1\ttrue\n",
            "(1)\t 731\t [ 1.196024  -0.9934919] \t0\tfalse\n",
            "(1)\t 732\t [ 1.196024  -0.9934919] \t0\tfalse\n",
            "(0)\t 733\t [ 2.212621  -2.1841125] \t0\ttrue\n",
            "(1)\t 734\t [ 1.7819543 -1.651508 ] \t0\tfalse\n",
            "(0)\t 735\t [ 1.673133  -1.7681091] \t0\ttrue\n",
            "(0)\t 736\t [ 2.0378776 -2.0639083] \t0\ttrue\n",
            "(0)\t 737\t [ 2.0625393 -2.0821173] \t0\ttrue\n",
            "(1)\t 738\t [ 2.3070579 -2.2117922] \t0\tfalse\n",
            "(1)\t 739\t [ 2.3070579 -2.2117922] \t0\tfalse\n",
            "(0)\t 740\t [ 0.47150397 -0.51338804] \t0\ttrue\n",
            "(1)\t 741\t [ 0.40601438 -0.27284545] \t0\tfalse\n",
            "(1)\t 742\t [ 0.40601438 -0.27284545] \t0\tfalse\n",
            "(0)\t 743\t [ 2.154263 -2.025956] \t0\ttrue\n",
            "(0)\t 744\t [ 0.11943164 -0.4210189 ] \t0\ttrue\n",
            "(1)\t 745\t [-0.97657037  0.7812351 ] \t1\ttrue\n",
            "(0)\t 746\t [ 2.1421716 -2.0850475] \t0\ttrue\n",
            "(0)\t 747\t [ 1.9515402 -2.0808992] \t0\ttrue\n",
            "(1)\t 748\t [ 2.302179  -2.2134492] \t0\tfalse\n",
            "(0)\t 749\t [ 1.9093069 -1.9934849] \t0\ttrue\n",
            "(0)\t 750\t [-0.5423504   0.43116176] \t1\tfalse\n",
            "(1)\t 751\t [ 0.28262407 -0.3572709 ] \t0\tfalse\n",
            "(1)\t 752\t [ 1.8121696 -1.7352895] \t0\tfalse\n",
            "(1)\t 753\t [ 2.241595  -2.1318944] \t0\tfalse\n",
            "(0)\t 754\t [ 2.2244332 -2.1833775] \t0\ttrue\n",
            "(0)\t 755\t [ 0.64994794 -0.8676117 ] \t0\ttrue\n",
            "(0)\t 756\t [ 1.9106328 -1.9900289] \t0\ttrue\n",
            "(0)\t 757\t [ 0.08757117 -0.15929817] \t0\ttrue\n",
            "(0)\t 758\t [-0.10741025 -0.02894628] \t1\tfalse\n",
            "(0)\t 759\t [ 1.3515228 -1.2307829] \t0\ttrue\n",
            "(0)\t 760\t [ 1.6160774 -1.6866131] \t0\ttrue\n",
            "(0)\t 761\t [ 1.9192643 -1.9186094] \t0\ttrue\n",
            "(0)\t 762\t [ 1.8397088 -1.9626852] \t0\ttrue\n",
            "(1)\t 763\t [-0.3836038  0.3276589] \t1\ttrue\n",
            "(1)\t 764\t [ 1.85139   -1.7905765] \t0\tfalse\n",
            "(0)\t 765\t [-2.131149   1.7444153] \t1\tfalse\n",
            "(0)\t 766\t [ 2.1811626 -2.1140258] \t0\ttrue\n",
            "(0)\t 767\t [ 2.156256  -2.0179727] \t0\ttrue\n",
            "(1)\t 768\t [ 2.0908358 -2.108978 ] \t0\tfalse\n",
            "(0)\t 769\t [ 1.0167944 -1.0398769] \t0\ttrue\n",
            "(0)\t 770\t [ 1.9592252 -2.1384888] \t0\ttrue\n",
            "(1)\t 771\t [ 2.1431165 -2.0817132] \t0\tfalse\n",
            "(1)\t 772\t [ 2.1431165 -2.0817132] \t0\tfalse\n",
            "(1)\t 773\t [ 1.640667  -1.7515513] \t0\tfalse\n",
            "(1)\t 774\t [ 1.640667  -1.7515513] \t0\tfalse\n",
            "(1)\t 775\t [ 2.195895  -2.1302676] \t0\tfalse\n",
            "(1)\t 776\t [ 2.195895  -2.1302676] \t0\tfalse\n",
            "(1)\t 777\t [ 1.4137442 -1.3262968] \t0\tfalse\n",
            "(0)\t 778\t [ 1.8884768 -1.6952269] \t0\ttrue\n",
            "(0)\t 779\t [-0.38618627  0.19702709] \t1\tfalse\n",
            "(0)\t 780\t [ 0.41722316 -0.33061254] \t0\ttrue\n",
            "(0)\t 781\t [ 1.566242  -1.7217395] \t0\ttrue\n",
            "(0)\t 782\t [ 1.9992539 -1.799195 ] \t0\ttrue\n",
            "(0)\t 783\t [ 0.73937815 -0.95450705] \t0\ttrue\n",
            "(0)\t 784\t [ 0.70134515 -0.8867376 ] \t0\ttrue\n",
            "(0)\t 785\t [ 0.2500073 -0.5490895] \t0\ttrue\n",
            "(0)\t 786\t [ 1.2053399 -0.990277 ] \t0\ttrue\n",
            "(1)\t 787\t [-1.6159612  1.2877736] \t1\ttrue\n",
            "(0)\t 788\t [ 2.0262413 -1.9688597] \t0\ttrue\n",
            "(0)\t 789\t [ 1.572632 -1.273633] \t0\ttrue\n",
            "(1)\t 790\t [ 0.58296514 -0.36953858] \t0\tfalse\n",
            "(1)\t 791\t [ 0.8809705 -0.8071207] \t0\tfalse\n",
            "(1)\t 792\t [ 2.095564  -1.9277849] \t0\tfalse\n",
            "(0)\t 793\t [-1.9636049  1.638773 ] \t1\tfalse\n",
            "(1)\t 794\t [ 0.32299376 -0.37933695] \t0\tfalse\n",
            "(1)\t 795\t [ 1.418783  -1.4394271] \t0\tfalse\n",
            "(0)\t 796\t [ 2.101682 -2.181501] \t0\ttrue\n",
            "(0)\t 797\t [ 1.9569521 -2.0360975] \t0\ttrue\n",
            "(0)\t 798\t [-1.1986324  1.0365043] \t1\tfalse\n",
            "(0)\t 799\t [-0.96227396  0.8723062 ] \t1\tfalse\n",
            "(0)\t 800\t [ 1.4100716 -1.2241187] \t0\ttrue\n",
            "(1)\t 801\t [ 1.9514074 -1.8812478] \t0\tfalse\n",
            "(1)\t 802\t [ 1.9514074 -1.8812478] \t0\tfalse\n",
            "(0)\t 803\t [ 1.3759229 -1.2710629] \t0\ttrue\n",
            "(0)\t 804\t [-1.5732484  1.3282857] \t1\tfalse\n",
            "(0)\t 805\t [-0.5014344   0.26398605] \t1\tfalse\n",
            "(0)\t 806\t [ 0.6246956 -0.4804275] \t0\ttrue\n",
            "(1)\t 807\t [ 0.7614757 -0.8521871] \t0\tfalse\n",
            "(0)\t 808\t [ 0.98355496 -1.1321182 ] \t0\ttrue\n",
            "(1)\t 809\t [ 0.28267142 -0.49834037] \t0\tfalse\n",
            "(1)\t 810\t [ 0.28267142 -0.49834037] \t0\tfalse\n",
            "(1)\t 811\t [ 1.4386125 -1.419131 ] \t0\tfalse\n",
            "(0)\t 812\t [ 1.6986957 -1.8046474] \t0\ttrue\n",
            "(1)\t 813\t [ 0.8210622  -0.53364193] \t0\tfalse\n",
            "(1)\t 814\t [ 0.8210622  -0.53364193] \t0\tfalse\n",
            "(1)\t 815\t [-2.0793242  1.6120422] \t1\ttrue\n",
            "(0)\t 816\t [ 0.23739246 -0.11080664] \t0\ttrue\n",
            "(0)\t 817\t [ 1.228327  -1.4237707] \t0\ttrue\n",
            "(0)\t 818\t [-0.37697572  0.4438374 ] \t1\tfalse\n",
            "(0)\t 819\t [ 1.8193667 -1.7601178] \t0\ttrue\n",
            "(0)\t 820\t [ 1.577487  -1.6017389] \t0\ttrue\n",
            "(0)\t 821\t [ 2.165423  -2.0385096] \t0\ttrue\n",
            "(1)\t 822\t [ 2.095965  -2.0006711] \t0\tfalse\n",
            "(0)\t 823\t [ 0.7324307  -0.49510154] \t0\ttrue\n",
            "(0)\t 824\t [ 1.7487187 -1.7046212] \t0\ttrue\n",
            "(0)\t 825\t [ 1.571816  -1.7506394] \t0\ttrue\n",
            "(0)\t 826\t [-0.7718128  0.5177644] \t1\tfalse\n",
            "(0)\t 827\t [0.09666499 0.04107006] \t0\ttrue\n",
            "(0)\t 828\t [ 2.092479  -2.0919201] \t0\ttrue\n",
            "(0)\t 829\t [ 0.39370358 -0.2685479 ] \t0\ttrue\n",
            "(0)\t 830\t [ 1.606004  -1.4908681] \t0\ttrue\n",
            "(1)\t 831\t [ 1.8283323 -1.9078661] \t0\tfalse\n",
            "(0)\t 832\t [ 1.974886  -1.8448789] \t0\ttrue\n",
            "(0)\t 833\t [ 1.4016081 -1.4111377] \t0\ttrue\n",
            "(1)\t 834\t [-0.8689225  0.8349292] \t1\ttrue\n",
            "(1)\t 835\t [ 0.09430829 -0.15840136] \t0\tfalse\n",
            "(1)\t 836\t [ 1.6064409 -1.546027 ] \t0\tfalse\n",
            "(1)\t 837\t [-0.02884405 -0.15757844] \t0\tfalse\n",
            "(0)\t 838\t [ 1.9732432 -1.9114201] \t0\ttrue\n",
            "(0)\t 839\t [ 2.2029333 -2.1446702] \t0\ttrue\n",
            "(0)\t 840\t [ 1.5528481 -1.6575645] \t0\ttrue\n",
            "(0)\t 841\t [-0.59483564  0.33001652] \t1\tfalse\n",
            "(0)\t 842\t [ 1.5922879 -1.7422292] \t0\ttrue\n",
            "(1)\t 843\t [ 2.1686954 -2.160013 ] \t0\tfalse\n",
            "(0)\t 844\t [-1.5744809  1.3710533] \t1\tfalse\n",
            "(1)\t 845\t [ 0.54858494 -0.46186534] \t0\tfalse\n",
            "(1)\t 846\t [0.18950123 0.0561401 ] \t0\tfalse\n",
            "(1)\t 847\t [ 1.2976058 -1.1461005] \t0\tfalse\n",
            "(1)\t 848\t [ 1.2976058 -1.1461005] \t0\tfalse\n",
            "(0)\t 849\t [-0.7035547  0.6594285] \t1\tfalse\n",
            "(1)\t 850\t [-0.16835102  0.02564784] \t1\ttrue\n",
            "(1)\t 851\t [ 1.3962213 -1.3958784] \t0\tfalse\n",
            "(1)\t 852\t [ 1.7190678 -1.4907572] \t0\tfalse\n",
            "(0)\t 853\t [ 1.7229676 -1.6198789] \t0\ttrue\n",
            "(1)\t 854\t [ 1.494076  -1.3248041] \t0\tfalse\n",
            "(0)\t 855\t [-0.22728568  0.08135087] \t1\tfalse\n",
            "(0)\t 856\t [-1.0721532  0.9711579] \t1\tfalse\n",
            "(0)\t 857\t [ 1.7302661 -1.8453574] \t0\ttrue\n",
            "(1)\t 858\t [ 1.932254  -1.9048096] \t0\tfalse\n",
            "(0)\t 859\t [-1.8777547  1.6171517] \t1\tfalse\n",
            "(1)\t 860\t [ 1.2443991 -1.3846579] \t0\tfalse\n",
            "(0)\t 861\t [ 0.92835593 -0.8600657 ] \t0\ttrue\n",
            "(1)\t 862\t [ 2.0580552 -1.9579973] \t0\tfalse\n",
            "(1)\t 863\t [ 2.0580552 -1.9579973] \t0\tfalse\n",
            "(1)\t 864\t [-0.33737952  0.4147484 ] \t1\ttrue\n",
            "(0)\t 865\t [ 0.6545637 -0.6745802] \t0\ttrue\n",
            "(0)\t 866\t [ 1.6219181 -1.4503   ] \t0\ttrue\n",
            "(0)\t 867\t [ 1.2852836 -1.2271236] \t0\ttrue\n",
            "(0)\t 868\t [-1.5538499  1.1845939] \t1\tfalse\n",
            "(0)\t 869\t [-0.27299625  0.2590196 ] \t1\tfalse\n",
            "(0)\t 870\t [ 1.8686422 -1.8439703] \t0\ttrue\n",
            "(0)\t 871\t [ 2.0816379 -1.8783451] \t0\ttrue\n",
            "(0)\t 872\t [ 1.5831189 -1.5928113] \t0\ttrue\n",
            "(1)\t 873\t [ 0.34765399 -0.10171571] \t0\tfalse\n",
            "(1)\t 874\t [ 1.2917025 -1.1812515] \t0\tfalse\n",
            "(1)\t 875\t [-0.7569803   0.49776572] \t1\ttrue\n",
            "(1)\t 876\t [ 1.718339  -1.8143983] \t0\tfalse\n",
            "(0)\t 877\t [ 2.3455942 -2.2134488] \t0\ttrue\n",
            "(1)\t 878\t [ 0.9955611  -0.82189614] \t0\tfalse\n",
            "(1)\t 879\t [-1.1208894  1.0351557] \t1\ttrue\n",
            "(1)\t 880\t [ 2.2035613 -2.0515926] \t0\tfalse\n",
            "(0)\t 881\t [-0.7128767  0.876029 ] \t1\tfalse\n",
            "(0)\t 882\t [-0.70376945  0.6260866 ] \t1\tfalse\n",
            "(1)\t 883\t [ 2.06307   -1.9234738] \t0\tfalse\n",
            "(1)\t 884\t [0.16453233 0.02053541] \t0\tfalse\n",
            "(1)\t 885\t [-1.2153684  1.07388  ] \t1\ttrue\n",
            "(0)\t 886\t [ 2.271251  -2.2370205] \t0\ttrue\n",
            "(0)\t 887\t [-1.2537711  1.1873629] \t1\tfalse\n",
            "(1)\t 888\t [-1.3786654  1.1932596] \t1\ttrue\n",
            "(0)\t 889\t [ 1.9179156 -1.9551587] \t0\ttrue\n",
            "(0)\t 890\t [ 1.2998967 -1.3907168] \t0\ttrue\n",
            "(0)\t 891\t [ 1.2296386 -1.2029352] \t0\ttrue\n",
            "(1)\t 892\t [ 1.7256716 -1.5139798] \t0\tfalse\n",
            "(1)\t 893\t [ 2.1269042 -2.0499427] \t0\tfalse\n",
            "(1)\t 894\t [ 2.100033 -1.979177] \t0\tfalse\n",
            "(1)\t 895\t [ 1.9683057 -1.7858193] \t0\tfalse\n",
            "(0)\t 896\t [-0.07890134 -0.2669703 ] \t0\ttrue\n",
            "(1)\t 897\t [ 1.2065297 -1.2207054] \t0\tfalse\n",
            "(1)\t 898\t [ 0.64992803 -0.6793545 ] \t0\tfalse\n",
            "(1)\t 899\t [-0.4253489   0.31373966] \t1\ttrue\n",
            "(1)\t 900\t [ 0.5878836  -0.36682275] \t0\tfalse\n",
            "Number of true predictions: 506\n",
            "Number of false predictions: 394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KIgsWWCTKdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1797a751-7dfc-4579-fbe6-2e464ab6e01e"
      },
      "source": [
        "print(\"Accuracy:\",true_count/count_line*100,\"%\")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 56.15982241953385 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMcHi1uLQdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print('True positives: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PmWUtXdLW1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "00571f3c-5664-4b2b-ee42-6893c42777b0"
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  \n",
        "\n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)\n",
        "  \n",
        " # print(pred_labels_i)\n",
        "\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI2K1bqaR5ng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb--na-oLbVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e01cbe97-17e2-40c8-af7d-cd0c0d55de96"
      },
      "source": [
        "matthews_set\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QowNXrYZSRMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "2aad8d56-d27b-4f9e-f8b9-f99d02e3fc6c"
      },
      "source": [
        "#for debugging purposes\n",
        "print(flat_predictions)\n",
        "print(\"************\")\n",
        "print(flat_true_labels)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1\n",
            " 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1\n",
            " 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
            " 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
            " 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 1 0 1\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
            " 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0\n",
            " 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1\n",
            " 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            "************\n",
            "[1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPTA_ePt0wVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model_location = input(\"Enter the location to save your model : \")    #example - task1hindi"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6aNsEuZMvjI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1da1ccd9-b86f-4ad1-fdea-d0eef4c521ae"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        "output_dir = '/content/drive/My Drive/FYP/bert/model-colab-'+taskid+language    #by default the model is stored here , you can change this location according to your convinience\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to /content/drive/My Drive/FYP/bert/model-colab-task1tamil\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/FYP/bert/model-colab-task1tamil/vocab.txt',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1tamil/special_tokens_map.json',\n",
              " '/content/drive/My Drive/FYP/bert/model-colab-task1tamil/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    }
  ]
}